{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517684ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c936bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c35f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_CE_KLD = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1781eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 20:10:59.317307: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.340970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.343844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f627b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591902c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 20:10:59.372996: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 20:10:59.373351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.374109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.374774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.928378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.929121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.929810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 20:10:59.930447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22312 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:06:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_gap = model_inceptionresnet_multigap()\n",
    "model_cnn = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\", trainable=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acf3fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ffd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOL PARAMETER TO SPECIFY IF THE TRAINING IS SPLITTED OR JOINT\n",
    "\n",
    "joint = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d622a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_good1 = []\n",
    "feats_MG = 'original'  \n",
    "feats_CNN = 'border_600x600'\n",
    "# feats_CNN_MG_PCA = 'cnn_mg_concat/pca_9744_auto'\n",
    "cnn = 'cnn_efficientnet_b7'\n",
    "\n",
    "\n",
    "def load_for_current_bad(i=0):\n",
    "    main_path=root_path + 'Data/AesthAI/alm/splitted/alm_train/' \n",
    "    feats_MG = 'original' \n",
    "    feats_CNN = 'border_600x600'\n",
    "    cnn = 'cnn_efficientnet_b7'\n",
    "\n",
    "    np.random.seed(i)\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    features_bad_list = []\n",
    "    for data in bad_data:\n",
    "        feat_path_1 = main_path + f'features/multigap/{feats_MG}/' + data['feature']\n",
    "        feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "        connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load(feat_path_2))))\n",
    "        features_bad_list.append(connected)\n",
    "    features_bad_list = np.squeeze(np.array(features_bad_list))\n",
    "    return features_bad_list\n",
    "\n",
    "\n",
    "def loading_all_data_from_json(main_path = root_path + 'Data/AesthAI/alm/splitted/alm_train/', joint=False):\n",
    "    features_good1 = []\n",
    "    features_bad_list = []\n",
    "    lbl = []\n",
    "    all_data = []   \n",
    "    feats_MG = 'original' \n",
    "    feats_CNN = 'border_600x600'\n",
    "    cnn = 'cnn_efficientnet_b7'\n",
    "\n",
    "    features_bad_list = list(map(load_for_current_bad,range(7)))\n",
    "\n",
    "        \n",
    "    alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "    good_data = json.load(alm_train_good)\n",
    "    for data in good_data:\n",
    "        feat_path_1 = main_path + f'features/multigap/{feats_MG}/' + data['feature']\n",
    "        feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "        connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load(feat_path_2))))\n",
    "        features_good1.append(connected)\n",
    "    features_good1 = np.squeeze(np.array(features_good1))\n",
    "    features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "    features_good, features_good_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "    \n",
    "    \n",
    "    val_data = np.concatenate((features_bad1_val, features_good_val), axis=0)\n",
    "    val_lbl = np.concatenate((np.zeros(len(features_bad1_val)), np.ones(len(features_good_val))), axis=0 )\n",
    "    val_data, val_lbl = shuffle(val_data, val_lbl)\n",
    "    \n",
    "    for i in range(7):\n",
    "        data_i = np.concatenate((features_good, features_bad_list[i]) , axis=0)\n",
    "        lbl_i = np.concatenate((np.ones(len(features_good)), np.zeros(len(features_bad_list[i]))), axis=0 )\n",
    "        data_i, lbl_i = shuffle(data_i,lbl_i)\n",
    "        all_data.append(data_i)\n",
    "        lbl.append(lbl_i)\n",
    "\n",
    "    all_data = np.array(all_data, dtype=object)\n",
    "    lbl = np.array(lbl, dtype=object)\n",
    "    \n",
    "    if joint:\n",
    "        clean_data = all_data[0]\n",
    "        for i in range(1, 7):\n",
    "            clean_data = np.vstack((clean_data, all_data[i]))\n",
    "        \n",
    "        clean_lbl = np.array([lbl[0]]).T\n",
    "        for i in range(1, 7):\n",
    "            clean_lbl = np.vstack((clean_lbl, np.array([lbl[i]]).T))\n",
    "            \n",
    "        return clean_data, clean_lbl, val_data, val_lbl\n",
    "    \n",
    "    return all_data, lbl, val_data, val_lbl  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bfb0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all = loading_all_data_from_json()\n",
    "all_data, all_lbl, val_data, val_lbl = loading_all_data_from_json(joint=joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ab6023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21182, 19488)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f261a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dummy\n",
    "def create_dummy(joint=joint):\n",
    "    \n",
    "    if not joint:\n",
    "        dummy_all_data = []\n",
    "        dummy_val_data = []\n",
    "        for i in range(7):\n",
    "            dummy_all_data.append(all_data[i][:, :100])\n",
    "        dummy_val_data = val_data[:, :100]\n",
    "\n",
    "    else:\n",
    "        dummy_all_data = all_data[:, :100]\n",
    "        dummy_val_data = val_data[:, :100]\n",
    "    \n",
    "    return dummy_all_data, dummy_val_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# all_data, val_data = create_dummy(joint=False) # IF YOU WANT TO USE DUMMY DATA\n",
    "# print(dummy_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6b32d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19364, 19488)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2beaaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_imgs_path = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark', 'images', 'good', '*'))\n",
    "bad_imgs_path = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark', 'images', 'bad', '*'))\n",
    "\n",
    "good_imgs_path_2 = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark2', 'images', 'good', '*'))\n",
    "bad_imgs_path_2 = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark2', 'images', 'bad', '*'))\n",
    "\n",
    "paths_bench = good_imgs_path + bad_imgs_path\n",
    "labels_bench = np.concatenate((np.ones(len(good_imgs_path)), np.zeros(len(bad_imgs_path))))\n",
    "\n",
    "paths_bench_2 = good_imgs_path_2 + bad_imgs_path_2\n",
    "labels_bench_2 = np.concatenate((np.ones(len(good_imgs_path_2)), np.zeros(len(bad_imgs_path_2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e7800",
   "metadata": {},
   "source": [
    "# Train funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac11ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_pairs(features_bad, features_good, train_size=0.9, shuffle=True):\n",
    "#     train_data_bad = features_bad\n",
    "#     print('train_data_bad shape', train_data_bad.shape)\n",
    "#     train_data_good = features_good\n",
    "#     print('train_data_good shape', train_data_good.shape)\n",
    "\n",
    "#     input_data = np.concatenate( (train_data_bad,train_data_good),axis=0)\n",
    "#     target_data = np.concatenate( (np.zeros(train_data_bad.shape[0]),np.ones(train_data_good.shape[0])) , axis=0 )\n",
    "    \n",
    "#     X_train, Y_train = shuffle(input_data,target_data)\n",
    "#     return X_train, Y_train\n",
    "\n",
    "\n",
    "\n",
    "def take_from_feats(data, pop):\n",
    "    idx = np.nonzero(pop)\n",
    "    new_data = np.squeeze(data[:, idx])\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def calc_acc(model, weights_path, X_test, y_test, batch_size):\n",
    "    '''\n",
    "    Compares Max classes with targets, getting mean class precision\n",
    "    '''\n",
    "    \n",
    "    model.load_weights(weights_path)\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    acc = (np.argmax(y_pred,axis=-1) == y_test).mean()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def trainer(model, data, weights_path, data_val, batch_size=128, epochs=30, learning_rate=0.03):\n",
    "    X_train, y_train = data\n",
    "    model.compile(loss=SC_CE_KLD,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=1e-07, decay=0, amsgrad=False))\n",
    "    model.load_weights(weights_path) \n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(weights_path, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=0, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    \n",
    "    \n",
    "    schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\n",
    "    \n",
    "    \n",
    "    callbacks_list = [checkpoint, schedule]\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=0,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data = data_val)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def training(data, data_val, joint=joint):\n",
    "    # creating model, and weights\n",
    "    if joint:\n",
    "        model = fc_model_softmax(input_num=(data[0].shape)[1]) \n",
    "    else:\n",
    "        model = fc_model_softmax(input_num=(data[0][0].shape)[1])\n",
    "    weights_path = f'models/Softmax/MultiGap_CNN_GA/pygad_popsize_8_vectorsize_5000_1_0.hdf5'#before training need to add weights path\n",
    "    model.save_weights(weights_path) #if we want to cancel learning and start from 0\n",
    "#     model.load_weights(weights_path_pretrained)\n",
    "#     weights_path = f'models/Softmax/MultiGap_CNN_GA/pygad_popsize_8_vectorsize_5000_1_0.hdf5'\n",
    "    \n",
    "    epochs = 15\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    if joint:\n",
    "        history = trainer(model, data, weights_path, data_val, batch_size, epochs, \n",
    "                      learning_rate=learning_rate)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        def trainer_i(i):\n",
    "            history = trainer(model, (data[0][i], data[1][i]), weights_path, data_val, batch_size, epochs, \n",
    "                              learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "        list(map(trainer_i, range(7)))\n",
    "    \n",
    "        \n",
    "    acc = calc_acc(model, weights_path, data_val[0], data_val[1], batch_size)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d51de9",
   "metadata": {},
   "source": [
    "# Eval funcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863d2481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, y=None, model_gap=None, model=None, model_cnn=None, curr_pop=None):\n",
    "    '''\n",
    "    Does prediction on given numpy image using\n",
    "    model_gap and model\n",
    "    '''\n",
    "    try:\n",
    "        feat_mg = model_gap.predict(x, verbose=0)\n",
    "    except:\n",
    "        x = x[None]\n",
    "        feat_mg = model_gap.predict(x, verbose=0)\n",
    "        \n",
    "    if model_cnn:\n",
    "        feat_cnn = model_cnn.predict(y, verbose=0)\n",
    "        feat = np.concatenate((np.squeeze(feat_mg), np.squeeze(feat_cnn)))\n",
    "        feat = feat[None]\n",
    "    else:\n",
    "        feat = feat_mg\n",
    "        \n",
    "    feat = take_from_feats(feat, curr_pop)[None]\n",
    "        \n",
    "    pred_score = model.predict(feat, verbose=0)\n",
    "\n",
    "    return pred_score\n",
    "    \n",
    "def predict_from_path(model_gap, model, paths, curr_pop=None, resize_func=None, size=None, for_all=False, save_results=None, \n",
    "                      save_to=None, model_cnn=None):\n",
    "    #always requires list of paths\n",
    "    predicted = []\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        img_mg = read_img(path=path, resize_func=resize_func, size=size, for_all=for_all)\n",
    "        img_cnn = None\n",
    "        if model_cnn:\n",
    "            img_cnn = read_img(path=path, resize_func=resize_add_border, size=(600, 600))\n",
    "        pred_score = predict(img_mg, img_cnn, model_gap, model, model_cnn, curr_pop)\n",
    "        predicted.append(pred_score)\n",
    "    \n",
    "    predicted = np.array(predicted)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    \n",
    "    if save_results:\n",
    "        np.save(save_to, np.argmax(predicted, axis=-1))\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "def calc_acc_eval(labels, predicted):\n",
    "    '''\n",
    "    Calculating mean class error, e.g. predicted classes are 1vs0, 0vs0, 0vs0, 0vs0, then we have acc=0.25\n",
    "    Inputs: \n",
    "        labels = target labels\n",
    "        predicted = predicted binary probability distribution for the input\n",
    "    Output:\n",
    "        mean class error\n",
    "    '''\n",
    "    acc = np.sum(np.array(labels) == np.argmax(np.array(predicted), axis=1)) / len(labels)\n",
    "    return np.round(acc * 100, 2)\n",
    "\n",
    "# def num_wrong_pred(labels, predicted):\n",
    "#     '''\n",
    "#     Calculating mean class error, e.g. predicted classes are 1vs0, 0vs0, 0vs0, 0vs0, then we have acc=0.25\n",
    "#     Inputs: \n",
    "#         labels = target labels\n",
    "#         predicted = predicted binary probability distribution for the input\n",
    "#     Output:\n",
    "#         mean class error\n",
    "#     '''\n",
    "#     wrong = len(labels) - np.sum(np.array(labels) == np.argmax(np.array(predicted), axis=1))\n",
    "#     return wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b72309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOL PARAMETER FOR FITNESS_FUNCTION!!!\n",
    "\n",
    "\n",
    "\n",
    "def is_similar(population):\n",
    "    pop_i = np.copy(population)\n",
    "    idx_nan = np.where(pop_i == np.zeros(shape=(np.array(pop_i).shape[0], np.array(pop_i).shape[1])))\n",
    "    pop_i[idx_nan[0], idx_nan[1]] = 5 # Hardcode\n",
    "    is_sim = (population[None] == pop_i[:, None]).mean(axis=-1)\n",
    "    idx = np.triu_indices(is_sim.shape[0])\n",
    "    idx1 = idx[0]\n",
    "    idx2 = idx[1]\n",
    "    new_idx = np.where((idx1 == idx2) == False)\n",
    "    idx1 = idx1[new_idx[0]]\n",
    "    idx2 = idx2[new_idx[0]]\n",
    "    idx = idx1, idx2\n",
    "    sim_pop_idxs = np.where(is_sim[idx] >= 0.8) # threshold\n",
    "    pop1 = idx1[sim_pop_idxs[0]]\n",
    "    pop2 = idx2[sim_pop_idxs[0]]\n",
    "    \n",
    "    return pop1, pop2\n",
    "\n",
    "def mutation_for_par(state, count_of_mut=100, similar_par=None):\n",
    "    indx_0 = np.where(state == 0)[1]\n",
    "    indx_1 = np.where(state == 1)[1]\n",
    "    if similar_par is not None:\n",
    "        idx_sim_1 = np.where(similar_par == 1)[1]\n",
    "        conf_matrix = indx_0[:, None] == idx_sim_1[None]\n",
    "        idx_to_del = np.where(conf_matrix)[0]\n",
    "        indx_0 = np.delete(indx_0, idx_to_del)\n",
    "        if len(indx_0) < count_of_mut:\n",
    "            print('Not mutated')\n",
    "            return state\n",
    "    mutate_idxs_0 = np.random.choice(indx_0, size=count_of_mut, replace=False)\n",
    "    mutate_idxs_1 = np.random.choice(indx_1, size=count_of_mut, replace=False)\n",
    "    state[0, mutate_idxs_0] = 1\n",
    "    state[0, mutate_idxs_1] = 0\n",
    "    \n",
    "    return state\n",
    "\n",
    "def mutation(offspring, ga_inctance):\n",
    "    idx = is_similar(offspring)\n",
    "    idx1 = np.unique(idx[0])\n",
    "    idx2 = np.unique(idx[1])\n",
    "    for idx_1, idx_2 in zip(idx1, idx2):\n",
    "        offspring[idx_1] = mutation_for_par(state=(offspring[idx_1])[None], \n",
    "                                        count_of_mut=2, similar_par=(offspring[idx_2])[None])\n",
    "    return offspring\n",
    "\n",
    "\n",
    "\n",
    "def fitness_function(chromosome, pop_idx):\n",
    "    print(np.sum(chromosome))\n",
    "    indexes = chromosome\n",
    "\n",
    "    if not joint:\n",
    "        \n",
    "        data_train = []# To avoid OVERWRITING DATASET\n",
    "\n",
    "        for i in range(7):\n",
    "            data_train.append(take_from_feats(all_data[i], indexes))\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        data_train = take_from_feats(all_data, indexes)        \n",
    "\n",
    "    data_val = take_from_feats(val_data, indexes)\n",
    "\n",
    "    \n",
    "\n",
    "    trainer = training((data_train, all_lbl), (data_val, val_lbl), joint=joint)\n",
    "\n",
    "    model = fc_model_softmax(input_num=np.sum(indexes))\n",
    "    weights_path = f'models/Softmax/MultiGap_CNN_GA/pygad_popsize_8_vectorsize_5000_1_0.hdf5'\n",
    "    model.load_weights(weights_path)\n",
    "    \n",
    "    predicted1 = predict_from_path(model_gap=model_gap, model=model, paths=paths_bench, curr_pop=indexes, resize_func=resize_max, \n",
    "                              size=(996, 996), for_all=False, model_cnn=model_cnn)\n",
    "    \n",
    "    acc1 = calc_acc_eval(labels_bench, predicted1)\n",
    "    print(f'Eval accuracy 1: {acc1} %')\n",
    "#     num_wrong_pred1 = num_wrong_pred(labels_bench, predicted1)\n",
    "    \n",
    "    predicted2 = predict_from_path(model_gap=model_gap, model=model, paths=paths_bench_2, curr_pop=indexes, resize_func=resize_max, \n",
    "                              size=(996, 996), for_all=False, model_cnn=model_cnn)\n",
    "    \n",
    "    acc2 = calc_acc_eval(labels_bench_2, predicted2)\n",
    "    print(f'Eval accuracy 2: {acc2} %')\n",
    "#     num_wrong_pred2 = num_wrong_pred(labels_bench_2, predicted2)\n",
    "\n",
    "    fx = 10*(100 - acc1)/25 + 10*(100 - acc2)/25 + (np.sum(indexes)/5000)\n",
    "    \n",
    "    best = np.load('best_res/best.npy')\n",
    "    acc1_best = best[0]\n",
    "    acc2_best = best[1] \n",
    "    \n",
    "    acc = (acc1 + acc2) / 2\n",
    "    print(f'fit score: {1/abs(7.5 - fx)}')\n",
    "    \n",
    "    if (acc1 > acc1_best and acc2 > acc2_best) or acc > 90:\n",
    "        np.save(f'best_res/best_solution_2_{acc1}_{acc2}', indexes)\n",
    "        model.save_weights(f'best_res/best_5k_2_{acc1}_{acc2}.hdf5')\n",
    "#         acc1_best = acc1\n",
    "#         acc2_best = acc2\n",
    "#         best = np.array([acc1_best, acc2_best])\n",
    "#         np.save('best_res/best_1_0', best)\n",
    "    \n",
    "    \n",
    "\n",
    "    return 1/abs(7.5 - fx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9567e3",
   "metadata": {},
   "source": [
    "# Pygad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee24dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'Genetic_Algorithm'\n",
      "C:\\Users\\CareAware\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Genetic_Algorithm\n"
     ]
    }
   ],
   "source": [
    "cd Genetic_Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4d84602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygad import GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61c572bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_pop(feat_vector_size, transformed_feat_vector_size, pop_size, init=False, init_size=3):\n",
    "    # generates random population\n",
    "    # INIT ONLY WHEN ORIGINAL DATA SHAPE! NOT DUMMY!\n",
    "    k = transformed_feat_vector_size // pop_size\n",
    "    path_list = glob(f'{root_path}/Projects/aesthetic-ai/DeepFL/DeepFL/best_res/best_solution*')\n",
    "    random.shuffle(path_list)\n",
    "    population = []\n",
    "    if init:\n",
    "        for i in range(init_size):\n",
    "            population.append(np.load(path_list[i]))\n",
    "        \n",
    "    len_pop = len(population)\n",
    "    \n",
    "    for i in range(1, pop_size - len_pop+1):\n",
    "        main_pop = np.concatenate((np.ones(transformed_feat_vector_size - i*k), np.zeros(feat_vector_size - transformed_feat_vector_size + i*k)))\n",
    "        np.random.shuffle(main_pop)\n",
    "        population.append(main_pop)\n",
    "        \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070ec486",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pop = generate_init_pop(19488, 5000, 12, init=True, init_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40f87a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_pop[11].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d9a7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaven/Projects/aesthetic-ai/DeepFL/DeepFL/pygad.py:581: UserWarning: The mutation_by_replacement parameter is set to True while the mutation_type parameter is not set to random but (<function mutation at 0x7f1a9c2a5820>). Note that the mutation_by_replacement parameter has an effect only when mutation_type='random'.\n",
      "  if not self.suppress_warnings: warnings.warn(\"The mutation_by_replacement parameter is set to True while the mutation_type parameter is not set to random but ({mut_type}). Note that the mutation_by_replacement parameter has an effect only when mutation_type='random'.\".format(mut_type=mutation_type))\n",
      "/home/zaven/Projects/aesthetic-ai/DeepFL/DeepFL/pygad.py:820: UserWarning: Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.\n",
      "  if not self.suppress_warnings: warnings.warn(\"Use the 'save_best_solutions' parameter with caution as it may cause memory overflow when either the number of generations or number of genes is large.\")\n"
     ]
    }
   ],
   "source": [
    "ga_instance = GA(num_generations=100,\n",
    "                       num_parents_mating=8,\n",
    "                       fitness_func=fitness_function,\n",
    "                       initial_population=init_pop,\n",
    "                       num_genes=len(init_pop[0]),\n",
    "                       gene_type=int,\n",
    "                       parent_selection_type=\"rws\",\n",
    "                       keep_parents=2,\n",
    "                       crossover_type=\"two_points\",\n",
    "                       mutation_type=mutation,\n",
    "                       mutation_by_replacement=True,\n",
    "                       random_mutation_min_val=0,\n",
    "                       random_mutation_max_val=1,\n",
    "                       save_best_solutions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38107179",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 20:13:03.721014: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-03 20:14:54.995961: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-11-03 20:14:56.444960: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-03 20:14:56.451208: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.2.300, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-11-03 20:14:56.452631: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2022-11-03 20:14:56.452661: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2022-11-03 20:14:56.452729: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: /usr/local/cuda/bin/ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1a9809a1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 23 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1a9809a1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval accuracy 1: 89.19 %\n",
      "Eval accuracy 2: 88.2 %\n",
      "fit score: 0.4233341800016931\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CHANGE FIT SCORE FROM ACC TO FX!!!!!!!! --> Done\n",
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375f925",
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = ga_instance.best_solutions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_instance.best_solutions_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb004c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
