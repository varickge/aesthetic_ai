{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b6007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/server3090ti/Projects/aesthetic-ai/DeepFL/DeepFL_binary\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e444a888",
   "metadata": {},
   "source": [
    "# check ToDos in def FindBestFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c73b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14739b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 11:34:40.950864: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.951336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.955146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.955571: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.956324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.956734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.958315: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-03 11:34:40.959374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.960084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:40.960782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:41.370031: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:41.370681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:41.371302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-03 11:34:41.371893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21666 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3c2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path()\n",
    "SC_CE_KLD = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff81a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_init_pop(feat_vector_size, transformed_feat_vector_size, pop_size, init=True, init_size=3):\n",
    "    # generates random population\n",
    "    # INIT ONLY WHEN ORIGINAL DATA SHAPE! NOT DUMMY!\n",
    "    k = transformed_feat_vector_size // pop_size\n",
    "    path_list = glob(f'{root_path}/Projects/aesthetic-ai/DeepFL/DeepFL_binary/best_res/best_solution*')\n",
    "    random.shuffle(path_list)\n",
    "    population = []\n",
    "    if init:\n",
    "        for i in range(init_size):\n",
    "            population.append(np.load(path_list[i]))\n",
    "        \n",
    "    len_pop = len(population)\n",
    "    \n",
    "    for i in range(1, pop_size - len_pop+1):\n",
    "        main_pop = np.concatenate((np.ones(transformed_feat_vector_size - i*k), np.zeros(feat_vector_size - transformed_feat_vector_size + i*k)))\n",
    "        np.random.shuffle(main_pop)\n",
    "        population.append(main_pop)\n",
    "        \n",
    "    return np.array(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b6ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = generate_init_pop(19488, 5000, 8, init=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e5f84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5000., 5000., 5000., 4375., 3750., 3125., 2500., 1875.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccd00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_from_feats(data, pop):\n",
    "    idx = np.nonzero(pop)\n",
    "    new_data = np.squeeze(data[:, idx])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b19499e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_for_current_bad(i=0):\n",
    "    main_path=root_path + 'Data/AesthAI/alm/splitted/alm_train/' \n",
    "    feats_MG = 'original' \n",
    "    feats_CNN = 'border_600x600'\n",
    "    cnn = 'cnn_efficientnet_b7'\n",
    "\n",
    "    np.random.seed(i)\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    features_bad_list = []\n",
    "    for data in bad_data:\n",
    "        feat_path_1 = main_path + f'features/multigap/{feats_MG}/' + data['feature']\n",
    "        feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "        connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load(feat_path_2))))\n",
    "        features_bad_list.append(connected)\n",
    "    features_bad_list = np.squeeze(np.array(features_bad_list))\n",
    "    return features_bad_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65661f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_all_data_from_json(main_path = root_path + 'Data/AesthAI/alm/splitted/alm_train/', joint=False):\n",
    "    features_good1 = []\n",
    "    lbl = []\n",
    "    data_all = []  \n",
    "    feats_MG = 'original' \n",
    "    feats_CNN = 'border_600x600'\n",
    "    cnn = 'cnn_efficientnet_b7'\n",
    "\n",
    "    features_bad_list = list(map(load_for_current_bad,range(7)))\n",
    "\n",
    "        \n",
    "    alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "    good_data = json.load(alm_train_good)\n",
    "    for data in good_data:\n",
    "        feat_path_1 = main_path + f'features/multigap/{feats_MG}/' + data['feature']\n",
    "        feat_path_2 = main_path + f'features/{cnn}/{feats_CNN}/' + data['feature']\n",
    "        connected = np.concatenate((np.squeeze(np.load(feat_path_1)), np.squeeze(np.load(feat_path_2))))\n",
    "        features_good1.append(connected)\n",
    "    features_good1 = np.squeeze(np.array(features_good1))\n",
    "    \n",
    "    features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "    features_good, features_good_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "    \n",
    "    \n",
    "    val_data = np.concatenate( (features_bad1_val, features_good_val ) , axis=0 )\n",
    "    val_lbl = np.concatenate( (np.zeros(len(features_bad1_val)), np.ones(len(features_good_val)) ), axis=0 )\n",
    "    val_data, val_lbl = shuffle(val_data,val_lbl)\n",
    "    \n",
    "    for i in range(7):\n",
    "        np.random.shuffle(features_good)\n",
    "        data_i = np.concatenate( (features_good, features_bad_list[i] ) , axis=0 )\n",
    "        lbl_i = np.concatenate( (np.ones(len(features_good)), np.zeros(len(features_bad_list[i])) ), axis=0 )\n",
    "        data_i, lbl_i = shuffle(data_i,lbl_i)\n",
    "        data_all.append(data_i)\n",
    "        lbl.append(lbl_i)\n",
    "        \n",
    "    data_all = np.array(data_all, dtype=object)\n",
    "    lbl = np.array(lbl, dtype=object)\n",
    "    print(lbl[0].shape)\n",
    "    \n",
    "       \n",
    "    if joint:\n",
    "        clean_data = data_all[0]\n",
    "        for i in range(1, 7):\n",
    "            clean_data = np.vstack((clean_data, data_all[i]))\n",
    "\n",
    "        clean_labels = np.array([lbl[0]]).T\n",
    "        for i in range(1, len(lbl)):\n",
    "            clean_labels = np.vstack((clean_labels, np.array([lbl[i]]).T))\n",
    "  \n",
    "    \n",
    "        return clean_data, clean_labels, val_data, val_lbl\n",
    "    return data_all, lbl, val_data, val_lbl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d16641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection( pop_acc, num_of_parents = 1):\n",
    "    if num_of_parents > len(pop_acc)  :\n",
    "        raise ValueError(f\"Number of pairs must be less than  len(pop_acc)  \")\n",
    "    sorted_parents = np.argsort(pop_acc)[: num_of_parents ]    #[:: -1]\n",
    "\n",
    "    print(sorted_parents)\n",
    "    return sorted_parents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31ccbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crossover(pair, k=2):\n",
    "#     parent1 = pair[0]\n",
    "#     parent2 = pair[1]\n",
    "#     print(parent1.shape)    \n",
    "#     cut = parent1.shape[0] // (k + 1)\n",
    "        \n",
    "#     child1 = np.array(parent1)\n",
    "#     child2 = np.array(parent2)\n",
    "#     part1 = parent1[cut: k * cut]\n",
    "#     part2 = parent2[cut: k * cut]\n",
    "\n",
    "#     child1[cut: k * cut] = part2\n",
    "#     child2[cut: k * cut] = part1\n",
    "\n",
    "#     return child1, child2\n",
    "# def crossover(pair, k=2):\n",
    "#     parent1 = pair[0]\n",
    "#     parent2 = pair[1]\n",
    "#     cut = len(parent1) // (k + 1)\n",
    "#     child1 = np.array(parent1)\n",
    "#     child2 = np.array(parent2)\n",
    "#     for i in range(k+1):\n",
    "#         if i % 2 == 0: # ToDo: change and take randomly maybe\n",
    "#             part1 = parent1[i * cut : (i + 1) * cut]\n",
    "#             part2 = parent2[i * cut : (i + 1) * cut]\n",
    "#             child1[i * cut : (i + 1) * cut] = part2\n",
    "#             child2[i * cut : (i + 1) * cut] = part1\n",
    "\n",
    "#     return child1, child2\n",
    "\n",
    "# def crossover(pair, k=2):\n",
    "#         parent1 = pair[0]\n",
    "#         parent2 = pair[1]\n",
    "#         size = parent1.sum()\n",
    "#         cut = len(parent1) // (k + 1)\n",
    "#         child1 = np.array(parent1, dtype=object)\n",
    "#         child2 = np.array(parent2, dtype=object)\n",
    "#         for i in range(k+1):\n",
    "#             if i % 2 == 0: # ToDo: change and take randomly maybe\n",
    "#                 part1 = parent1[i * cut : (i + 1) * cut]\n",
    "#                 part2 = parent2[i * cut : (i + 1) * cut]\n",
    "#                 child1[i * cut : (i + 1) * cut] = part2\n",
    "#                 child2[i * cut : (i + 1) * cut] = part1\n",
    "#         if child1.sum() != size:\n",
    "#             change = child1.sum() - size\n",
    "#             if change > 0:\n",
    "#                 indx = np.nonzero(child1)[0]\n",
    "#                 inx_to_change = np.random.choice(indx, size=int(abs(change)), replace=False)\n",
    "#                 child1[inx_to_change] = 0 \n",
    "#             else:\n",
    "#                 indx = np.where(child1 == 0)[0]\n",
    "#                 inx_to_change = np.random.choice(indx, size=int(abs(change)), replace=False)\n",
    "#                 child1[inx_to_change] = 1\n",
    "            \n",
    "#         if child2.sum() != size:\n",
    "#             change = child2.sum() - size\n",
    "#             if change > 0:\n",
    "#                 indx = np.nonzero(child2)[0]\n",
    "#                 inx_to_change = np.random.choice(indx, size=int(abs(change)), replace=False)\n",
    "#                 child2[inx_to_change] = 0 \n",
    "#             else:\n",
    "#                 indx = np.where(child2 == 0)[0]\n",
    "#                 inx_to_change = np.random.choice(indx, size=int(abs(change)), replace=False)\n",
    "#                 child2[inx_to_change] = 1   \n",
    "# #         if np.unique(child1).shape[0] != child1.shape[0]:\n",
    "# #             child1 = list(set(child1))\n",
    "# #             repeats = np.delete(np.arange(19488), child1 )\n",
    "# #             size = parent1.shape[0] - len(child1)\n",
    "# #             add_elem = np.random.choice(repeats, size, False)\n",
    "# #             child1.extend(add_elem)\n",
    "# #             child1 = np.asarray(child1)\n",
    "\n",
    "# #         if np.unique(child2).shape[0] != child2.shape[0]:\n",
    "# #             child2 = list(set(child2))\n",
    "# #             repeats = np.delete(np.arange(19488), child2 )\n",
    "# #             size = parent2.shape[0] - len(child2)\n",
    "# #             add_elem = np.random.choice(repeats, size, False)\n",
    "# #             child2.extend(add_elem)\n",
    "# #             child2 = np.asarray(child2)\n",
    "        \n",
    "#         return child1, child2\n",
    "def crossover(matrix, k=2):\n",
    "    sum_chromosome = np.sum(matrix[0])\n",
    "    print(sum_chromosome)\n",
    "    if matrix.shape[0] % 2 != 0:\n",
    "        raise ValueError('Number of parents for crossover must be even!')\n",
    "    matrix = matrix.reshape(int(matrix.shape[0]/2), 2, matrix.shape[1])\n",
    "    tail_len = matrix.shape[2] % (k + 1)\n",
    "    splitted_1 = np.split(matrix[:, 0, :matrix.shape[2]-tail_len], k+1, axis=1)\n",
    "    splitted_1[len(splitted_1)-1] = np.concatenate((splitted_1[len(splitted_1)-1], matrix[:, 0, matrix.shape[2]-tail_len:]), axis=1)\n",
    "    splitted_2 = np.split(matrix[:, 1, :matrix.shape[2]-tail_len], k+1, axis=1)\n",
    "    splitted_2[len(splitted_2)-1] = np.concatenate((splitted_2[len(splitted_2)-1], matrix[:, 1, matrix.shape[2]-tail_len:]), axis=1)\n",
    "    child_split_1 = splitted_1.copy()\n",
    "    child_split_2 = splitted_2.copy()\n",
    "    child_split_1[::2] = splitted_2[::2]\n",
    "    child_split_2[::2] = splitted_1[::2]\n",
    "    child_split_1 = np.concatenate(child_split_1, axis=1)\n",
    "    child_split_2 = np.concatenate(child_split_2, axis=1)\n",
    "    child_stack = np.stack((child_split_1, child_split_2), axis=2)\n",
    "    child_concat_split = np.split(child_stack, child_stack.shape[0], axis=0)\n",
    "    child_concat = np.concatenate(child_concat_split, axis=2)\n",
    "    offspring = np.array(np.squeeze(child_concat.T))\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8739b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_population(pop,pop_acc,children):\n",
    "    indx_to_change = np.argsort(pop_acc)[:len(children)]\n",
    "    pop[indx_to_change] = children\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8716fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similar(population):\n",
    "    pop_i = np.copy(population)\n",
    "    idx_nan = np.where(pop_i == np.zeros(shape=(np.array(pop_i).shape[0], np.array(pop_i).shape[1])))\n",
    "    pop_i[idx_nan[0], idx_nan[1]] = 5 # Hardcode\n",
    "    is_sim = (population[None] == pop_i[:, None]).mean(axis=-1)\n",
    "    idx = np.triu_indices(is_sim.shape[0])\n",
    "    idx1 = idx[0]\n",
    "    idx2 = idx[1]\n",
    "    new_idx = np.where((idx1 == idx2) == False)\n",
    "    idx1 = idx1[new_idx[0]]\n",
    "    idx2 = idx2[new_idx[0]]\n",
    "    idx = idx1, idx2\n",
    "    sim_pop_idxs = np.where(is_sim[idx] >= 0.8) # 0.8 is threshold\n",
    "    pop1 = idx1[sim_pop_idxs[0]]\n",
    "    pop2 = idx2[sim_pop_idxs[0]]\n",
    "    \n",
    "    return pop1, pop2\n",
    "\n",
    "def mutation_for_par(state, count_of_mut=100, similar_par=None):\n",
    "    indx_0 = np.where(state == 0)[1]\n",
    "    indx_1 = np.where(state == 1)[1]\n",
    "    if similar_par is not None:\n",
    "        idx_sim_1 = np.where(similar_par == 1)[1]\n",
    "        conf_matrix = indx_0[:, None] == idx_sim_1[None]\n",
    "        idx_to_del = np.where(conf_matrix)[0]\n",
    "        indx_0 = np.delete(indx_0, idx_to_del)\n",
    "        if len(indx_0) < count_of_mut:\n",
    "            print('Not mutated')\n",
    "            return state\n",
    "    mutate_idxs_0 = np.random.choice(indx_0, size=count_of_mut, replace=False)\n",
    "    mutate_idxs_1 = np.random.choice(indx_1, size=count_of_mut, replace=False)\n",
    "    state[0, mutate_idxs_0] = 1\n",
    "    state[0, mutate_idxs_1] = 0\n",
    "    \n",
    "    return state\n",
    "\n",
    "def mutation(offspring):\n",
    "    idx = is_similar(offspring)\n",
    "    idx1 = np.unique(idx[0])\n",
    "    idx2 = np.unique(idx[1])\n",
    "    for idx_1, idx_2 in zip(idx1, idx2):\n",
    "        offspring[idx_1] = mutation_for_par(state=(offspring[idx_1])[None], \n",
    "                                        count_of_mut=2, similar_par=(offspring[idx_2])[None])\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc80e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.048\n",
    "    return lr * np.exp(-k*epoch)\n",
    "\n",
    "def calc_acc_training(model, weights_path, X_test, y_test, batch_size):\n",
    "    '''\n",
    "    Compares Max classes with targets, getting mean class precision\n",
    "    '''\n",
    "    model.load_weights(weights_path)\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "\n",
    "    acc = (np.argmax(y_pred,axis=-1) == y_test).mean()\n",
    "    \n",
    "    \n",
    "    return acc\n",
    "\n",
    "def trainer(model, data, weights_path, data_val, batch_size=128, epochs=10, learning_rate=0.003):\n",
    "\n",
    "    X_train, y_train = data\n",
    "\n",
    "#     train_dataset = tf.data.Dataset.from_tensor_slices((np.array(X_train), np.array(y_train))).batch(batch_size)\n",
    "    \n",
    "    model.compile(loss=SC_CE_KLD,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=1e-07, decay=0, amsgrad=False))\n",
    "    model.load_weights(weights_path) \n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(weights_path, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=0, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    \n",
    "    \n",
    "    schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\n",
    "    \n",
    "    callbacks_list = [checkpoint, schedule]\n",
    "    with tf.device('/GPU:1'):\n",
    "        history = model.fit(X_train, y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=0,\n",
    "                            callbacks=callbacks_list,\n",
    "                            validation_data = data_val)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc598be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(data, data_val, model, weights_path, \n",
    "             batch_size, epochs, learning_rate):\n",
    "#     # creating model, and weights\n",
    "#     model = fc_model_softmax(input_num=(data[0][0].shape)[0]) #ToDo: check is all ok with shapes here --> checked\n",
    "#     weights_path_pretrained = f'models/MultiGap_CNN_GA/custom_ga_popsize_10_vectorsize_5000.hdf5'#before training need to add weights path\n",
    "#     model.save_weights(weights_path_pretrained) #if we want to cancel learning and start from 0\n",
    "#     model.load_weights(weights_path_pretrained)\n",
    "#     weights_path =  f'models/MultiGap_CNN_GA/custom_ga_popsize_10_vectorsize_5000.hdf5'\n",
    "    \n",
    "#     # choosing hyperparams\n",
    "#     epochs = 15\n",
    "#     batch_size = 128\n",
    "#     learning_rate = 0.003\n",
    "    \n",
    "    # Training FC process\n",
    "    history = trainer(model, data, weights_path, data_val, batch_size, epochs, \n",
    "                          learning_rate=learning_rate)\n",
    "    acc = calc_acc_training(model, weights_path, data_val[0], data_val[1], batch_size)\n",
    "    \n",
    "#     print('----- Accuracy =', acc, ' -----')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfcc1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: need to change these functions, there is no need pca, normalize, y --> checked\n",
    "# we need y, because we use different resize functions for mg and cnn\n",
    "def predict(x, y=None, model_gap=None, model=None, model_cnn=None, curr_pop=None):\n",
    "    '''\n",
    "    Does prediction on given numpy image using\n",
    "    model_gap and model\n",
    "    '''\n",
    "    try:\n",
    "\n",
    "        feat_mg = model_gap.predict(x, verbose=0)\n",
    "    except:\n",
    "        x = x[None]\n",
    "        feat_mg = model_gap.predict(x, verbose=0)\n",
    "        \n",
    "    if model_cnn:\n",
    "        feat_cnn = model_cnn.predict(y, verbose=0)\n",
    "        feat = np.concatenate((np.squeeze(feat_mg), np.squeeze(feat_cnn)))\n",
    "        feat = feat[None]\n",
    "    else:\n",
    "        feat = feat_mg\n",
    "        \n",
    "    if curr_pop is not None:\n",
    "        feat = take_from_feats(feat, curr_pop)\n",
    "        \n",
    "    feat = feat[None]\n",
    "    pred_score = model.predict(feat, verbose=0)\n",
    "\n",
    "    return pred_score\n",
    "    \n",
    "def predict_from_path(model_gap, model, paths, curr_pop = None, resize_func=None, size=None, for_all=False, save_results=None, \n",
    "                      save_to=None, model_cnn=None):\n",
    "    #always requires list of paths\n",
    "    predicted = []\n",
    "    \n",
    "    for i, path in enumerate(paths):\n",
    "        img_mg = read_img(path=path, resize_func=resize_func, size=size, for_all=for_all)\n",
    "        img_cnn = None\n",
    "        if model_cnn:\n",
    "            img_cnn = read_img(path=path, resize_func=resize_add_border, size=(600, 600))\n",
    "        pred_score = predict(img_mg, img_cnn, model_gap, model, model_cnn, curr_pop)\n",
    "        predicted.append(pred_score)\n",
    "    \n",
    "    predicted = np.array(predicted)\n",
    "    predicted = np.squeeze(predicted)\n",
    "    \n",
    "    if save_results:\n",
    "        np.save(save_to, np.argmax(predicted, axis=-1))\n",
    "        \n",
    "    return predicted\n",
    "\n",
    "def calc_acc_eval(labels, predicted):\n",
    "    '''\n",
    "    Calculating mean class error, e.g. predicted classes are 1vs0, 0vs0, 0vs0, 0vs0, then we have acc=0.25\n",
    "    Inputs: \n",
    "        labels = target labels\n",
    "        predicted = predicted binary probability distribution for the input\n",
    "    Output:\n",
    "        mean class error\n",
    "    '''\n",
    "    acc = np.sum(np.array(labels) == np.argmax(np.array(predicted), axis=1)) / len(labels)\n",
    "    \n",
    "    return np.round(acc * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b724f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestFeats(pop_size, transformed_feat_vector_size, \n",
    "                  max_gen=50, num_of_parents=4, joint=False):\n",
    "\n",
    "    good_imgs_path = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark', 'images', 'good', '*'))\n",
    "    bad_imgs_path = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark', 'images', 'bad', '*'))\n",
    "\n",
    "    good_imgs_path_2 = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark2', 'images', 'good', '*'))\n",
    "    bad_imgs_path_2 = glob(os.path.join(root_path, 'Data', 'AesthAI', 'benchmark2', 'images', 'bad', '*'))\n",
    "    \n",
    "    paths_bench = good_imgs_path + bad_imgs_path\n",
    "    labels_bench = np.concatenate((np.ones(len(good_imgs_path)), np.zeros(len(bad_imgs_path))))\n",
    "\n",
    "    paths_bench_2 = good_imgs_path_2 + bad_imgs_path_2\n",
    "    labels_bench_2 = np.concatenate((np.ones(len(good_imgs_path_2)), np.zeros(len(bad_imgs_path_2))))\n",
    "    with tf.device('/GPU:1'):\n",
    "        model_gap = model_mg = model_inceptionresnet_multigap()   \n",
    "        model_cnn = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",trainable=False) ])\n",
    "    generation = 0\n",
    "#     pop_acc = np.zeros(pop_size)\n",
    "    data, lbl, val_data, val_lbl = loading_all_data_from_json(joint=joint)\n",
    "    \n",
    "    print('data loaded')\n",
    "    \n",
    "    def cycle_for_population(pop, model_gap=model_gap, paths_bench=paths_bench, paths_bench_2=paths_bench_2,\n",
    "                        resize_max=resize_max, model_cnn=model_cnn, labels_bench=labels_bench, labels_bench2=labels_bench_2,\n",
    "                        joint=joint, transformed_feat_vector_size = transformed_feat_vector_size):\n",
    "        probs = [1,1,5]  \n",
    "#         map(trainer_for_pair, range(7))\n",
    "#         print('training finished')\n",
    "        model = fc_model_softmax(input_num=int(pop.sum()))\n",
    "        weights_path =  f'models/Softmax/MultiGap_CNN_GA/custom_ga_popsize_10_vectorsize_5000.hdf5' \n",
    "        model.save_weights(weights_path) #if we want to cancel learning and start from 0\n",
    "        #ToDo: imast ka weighter sarqelu heto load anelu stex ? menak sarqely heriqa erevi!!! anunnel pretrained petq chi, bayc stugel nor anel\n",
    "\n",
    "#         weights_path = f'models/Softmax/MultiGap_CNN_GA/custom_ga_popsize_10_vectorsize_5000.hdf5'  \n",
    "#         model.load_weights(weights_path)\n",
    "        \n",
    "        # choosing hyperparams\n",
    "        epochs = 15 # <---- be attentive\n",
    "        batch_size = 128\n",
    "        learning_rate = 0.003\n",
    "        \n",
    "        if joint:\n",
    "#             print( \"joint is true\")\n",
    "            curr_pop = pop\n",
    "            data_transformed = take_from_feats(data, curr_pop)\n",
    "            val_data_transformed = take_from_feats(val_data, curr_pop)\n",
    "            acc_t = training((data_transformed, lbl), (val_data_transformed, val_lbl),\n",
    "                                  model=model, weights_path=weights_path, batch_size=batch_size, \n",
    "                                 epochs=epochs, learning_rate=learning_rate)\n",
    "            \n",
    "        else:\n",
    "#             print( \"joint is false\")\n",
    "            def trainer_for_pair(i, data=data,curr_pop=pop, val_data=val_data, lbl=lbl, val_lbl=val_lbl):\n",
    "                data_transformed = take_from_feats(data[i], curr_pop)\n",
    "                val_data_transformed = take_from_feats(val_data, curr_pop)\n",
    "                acc_t = training((data_transformed, lbl[i]), (val_data_transformed, val_lbl),\n",
    "                                  model=model, weights_path=weights_path, batch_size=batch_size, \n",
    "                                 epochs=epochs, learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "            list(map(trainer_for_pair, range(7)))\n",
    "            \n",
    "#         model = fc_model_softmax(input_num=int(sum(pop))) #ToDo: imasty ka 2 angam model sarqelu u 2 angam weightery load anelu?\n",
    "#         model.load_weights(weights_path)\n",
    "        \n",
    "        predicted1 = predict_from_path(model_gap=model_gap, model=model, paths=paths_bench, curr_pop=pop, resize_func=resize_max, \n",
    "                                  size=(996, 996), for_all=False, model_cnn=model_cnn)\n",
    "\n",
    "        acc1 = calc_acc_eval(labels_bench, predicted1)\n",
    "    #     print(f'Accuracy: {acc1} %')\n",
    "    #     print('Done eval 1')\n",
    "\n",
    "        predicted2 = predict_from_path(model_gap=model_gap, model=model, paths=paths_bench_2, curr_pop=pop, resize_func=resize_max, \n",
    "                                  size=(996, 996), for_all=False, model_cnn=model_cnn)\n",
    "\n",
    "        acc2 = calc_acc_eval(labels_bench_2, predicted2)\n",
    "        \n",
    "        best = np.load('best_res/best.npy')\n",
    "        acc1_best = best[0]\n",
    "        acc2_best = best[1] \n",
    "\n",
    "        if acc1 > acc1_best and acc2 > acc2_best or (acc1+acc2)/2 > 90:\n",
    "            np.save(f'best_res/best_solution_custom_{acc1}_{acc2}', indexes)\n",
    "            model.save_weights(f'best_res/best_custom_{acc1}_{acc2}.hdf5')\n",
    "#             acc1_best = acc1\n",
    "#             acc2_best = acc2\n",
    "#             best = np.array([acc1_best, acc2_best])\n",
    "#             np.save('best_res/best', best)\n",
    "\n",
    "    #     print(f'Accuracy: {acc1} %')\n",
    "    #     print('Done eval 2')\n",
    "        print(f'Acc bench1: {acc1}\\nAcc bench2: {acc2}\\nMean: {(acc1+acc2)/2}')\n",
    "        fit_for_pop = probs[0] * (100 - acc1) + probs[1] * (100 - acc2) + probs[2] * pop.sum() / transformed_feat_vector_size\n",
    "        print(f'fit_for_pop {fit_for_pop}')\n",
    "        return    fit_for_pop #(acc1+acc2)/2\n",
    "    \n",
    "    if joint:\n",
    "        pop = generate_init_pop(feat_vector_size=data.shape[1], \n",
    "                              transformed_feat_vector_size=transformed_feat_vector_size,\n",
    "                              pop_size=pop_size)\n",
    "        \n",
    "    else:\n",
    "        print(data[0].shape[1])\n",
    "        pop = generate_init_pop(feat_vector_size=data[0].shape[1],\n",
    "                              transformed_feat_vector_size=transformed_feat_vector_size,\n",
    "                              pop_size=pop_size)\n",
    "#     pop = np.asarray(pop)\n",
    "    \n",
    "    while generation < max_gen:\n",
    "        pop_acc = list(map(cycle_for_population,pop)) \n",
    "        parent_indx = selection(pop_acc, num_of_parents=num_of_parents)\n",
    "        parents = pop[parent_indx]  #np.array(pop[indx] for indx in parent_indx)\n",
    "        print(parents)\n",
    "        print(parents.shape)\n",
    "#         children = []\n",
    "#         for pair in parent_pairs:\n",
    "#             children.extend(crossover((pop[pair[0]],pop[pair[1]])))\n",
    "#         parents = np.squeeze(parents, axis=0)\n",
    "        print(parents)\n",
    "    \n",
    "        children = crossover(parents)\n",
    "        children_mutation = mutation(children)  \n",
    "        pop = np.array(pop)\n",
    "        pop = new_population(pop, pop_acc, children_mutation)\n",
    "            \n",
    "        #ToDo: selection function --> checked\n",
    "        #ToDo: add crossover func --> checked\n",
    "        #ToDo: add mutation func --> checked\n",
    "        #ToDo: change population function\n",
    "        \n",
    "        \n",
    "        generation += 1\n",
    "        print(f'generation {generation}')\n",
    "    return pop[np.argsort(pop_acc)[-1]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897d4f42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19364,)\n",
      "data loaded\n",
      "19488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-03 11:35:38.673487: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2022-11-03 11:36:49.862962: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2022-11-03 11:36:50.309192: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-03 11:36:50.310047: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-03 11:36:50.310060: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2022-11-03 11:36:50.310529: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-11-03 11:36:50.310576: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc bench1: 87.39\n",
      "Acc bench2: 89.46\n",
      "Mean: 88.425\n",
      "fit_for_pop 28.150000000000006\n",
      "Acc bench1: 85.59\n",
      "Acc bench2: 88.97\n",
      "Mean: 87.28\n",
      "fit_for_pop 30.439999999999998\n",
      "Acc bench1: 89.19\n",
      "Acc bench2: 87.43\n",
      "Mean: 88.31\n",
      "fit_for_pop 28.379999999999995\n",
      "Acc bench1: 79.28\n",
      "Acc bench2: 84.53\n",
      "Mean: 81.905\n",
      "fit_for_pop 40.774\n",
      "Acc bench1: 80.18\n",
      "Acc bench2: 87.23\n",
      "Mean: 83.70500000000001\n",
      "fit_for_pop 36.75799999999999\n",
      "Acc bench1: 84.68\n",
      "Acc bench2: 87.91\n",
      "Mean: 86.295\n",
      "fit_for_pop 31.161999999999995\n",
      "Acc bench1: 82.88\n",
      "Acc bench2: 87.23\n",
      "Mean: 85.055\n",
      "fit_for_pop 33.226\n",
      "Acc bench1: 75.68\n",
      "Acc bench2: 85.78\n",
      "Mean: 80.73\n",
      "fit_for_pop 41.459999999999994\n",
      "Acc bench1: 84.68\n",
      "Acc bench2: 88.68\n",
      "Mean: 86.68\n",
      "fit_for_pop 29.143999999999988\n",
      "Acc bench1: 86.49\n",
      "Acc bench2: 87.52\n",
      "Mean: 87.005\n",
      "fit_for_pop 28.07800000000001\n",
      "Acc bench1: 79.28\n",
      "Acc bench2: 85.78\n",
      "Mean: 82.53\n",
      "fit_for_pop 36.611999999999995\n"
     ]
    }
   ],
   "source": [
    "best_feats = findBestFeats(pop_size = 12, transformed_feat_vector_size=5000, \n",
    "              max_gen=50, num_of_parents=8, joint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbaa04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
