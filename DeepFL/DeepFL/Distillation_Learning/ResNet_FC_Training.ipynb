{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac835ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:31:31.006409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:31.006878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:31.010891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:31.011326: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:31.011943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:31.012349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from final_utils import *\n",
    "\n",
    "gpu = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpu[0], True)\n",
    "\n",
    "from ResNet_FC import *\n",
    "from ResNet import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bb24e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:31:35.765711: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 14:31:35.767038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:35.767870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:35.768615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:36.216412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:36.217066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:36.217653: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-06 14:31:36.218217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15092 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#     try:\n",
    "#         tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "#         logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Visible devices must be set before GPUs have been initialized\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b60bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path()\n",
    "SC_CE_KLD = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac32fd",
   "metadata": {},
   "source": [
    "### DataLoader and other nesseccary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fb85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(y_true, y_pred): \n",
    "    return ((tf.math.argmax(y_pred, axis=1)) == tf.cast(y_true, dtype=tf.dtypes.int64)).numpy().mean()\n",
    "\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.04\n",
    "    return lr * np.exp(-k*epoch)\n",
    "\n",
    "\n",
    "def load_batch(paths):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i,path in enumerate(paths):\n",
    "        try:\n",
    "            img = cv2.imread(path, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype('float32')\n",
    "            img /= 255 \n",
    "            if 'good' in path:\n",
    "                label = 1\n",
    "            elif 'bad' in path:\n",
    "                label = 0\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "    images = np.stack(images)\n",
    "\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd814f3b",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e096aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, data, data_val,batch_size=128, epochs=50, learning_rate=0.03, verbose=0):\n",
    "    X_train, y_train = load_batch(data)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)#.prefetch(tf.data.AUTOTUNE)  # ToDo: here use .cache()\n",
    "\n",
    "    history = model.fit(train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=1,\n",
    "                        verbose=verbose,\n",
    "                        validation_data = data_val)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894e5c7",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f61a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main_path = main_path=f'{root_path}Data/AesthAI/alm/splitted/alm_train/'\n",
    "# paths_bad = []\n",
    "# paths_good = []\n",
    "    \n",
    "# for i in range(7):\n",
    "#     alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "#     bad_data = json.load(alm_train_bad)\n",
    "    \n",
    "#     for data in bad_data:\n",
    "#         path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "#         paths_bad.append(path_to_img)\n",
    "        \n",
    "# alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "# good_data = json.load(alm_train_good)\n",
    "# for data in good_data:\n",
    "#     path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "#     paths_good.append(path_to_img)\n",
    "    \n",
    "# for i in range(7):\n",
    "#     paths_bad[i] = np.squeeze(np.array(paths_bad[i]))\n",
    "# paths_good = np.squeeze(np.array(paths_good))\n",
    "   \n",
    "# # Generating static validation data\n",
    "# paths_bad, paths_bad_val = extract_static_val_data(paths_bad, perc = 0.017) #original - 0.017\n",
    "# paths_good, paths_good_val = extract_static_val_data(paths_good, perc = 0.11) #original - 0.11\n",
    "\n",
    "# paths_bad = np.array(paths_bad)\n",
    "# paths_good = np.array(paths_good)\n",
    "# paths_bad_val = np.array(paths_bad_val)\n",
    "# paths_good_val = np.array(paths_good_val)\n",
    "# labels_good = np.ones(len(paths_good))\n",
    "# labels_bad = np.zeros(len(paths_bad))\n",
    "\n",
    "main_path = main_path=f'{root_path}Data/AesthAI/alm/splitted/alm_train/'\n",
    "paths_bad = []\n",
    "paths_good = []\n",
    "    \n",
    "for i in range(7):\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    \n",
    "    for data in bad_data:\n",
    "        path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "        paths_bad.append(path_to_img)\n",
    "        \n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "    paths_good.append(path_to_img)\n",
    "    \n",
    "for i in range(7):\n",
    "    paths_bad[i] = np.squeeze(np.array(paths_bad[i]))\n",
    "paths_good = np.squeeze(np.array(paths_good))\n",
    "   \n",
    "# Generating static validation data\n",
    "paths_bad, paths_bad_val = extract_static_val_data(paths_bad, perc = 0.014) #original - 0.017\n",
    "paths_good, paths_good_val = extract_static_val_data(paths_good, perc = 0.04) #original - 0.11\n",
    "\n",
    "paths_bad = np.array(paths_bad)\n",
    "paths_bad_val = np.array(paths_bad_val)\n",
    "paths_good = np.array(paths_good)\n",
    "paths_good_val = np.array(paths_good_val)\n",
    "\n",
    "full_data = np.concatenate((np.repeat(paths_good, 7), paths_bad))\n",
    "    \n",
    "#shuffling\n",
    "idx = np.random.permutation(len(full_data))\n",
    "full_data = full_data[idx]\n",
    "# full_data = full_data[:5000]  # debug\n",
    "paths_good_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae74d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_data = np.concatenate((np.repeat(paths_good, 7), paths_bad))\n",
    "# full_labels = np.concatenate(((np.repeat(labels_good, 7), labels_bad)))   \n",
    "\n",
    "# #shuffling\n",
    "# idx = np.random.permutation(len(full_data))\n",
    "# full_data = full_data[idx]\n",
    "# print(full_data.shape)\n",
    "# print(full_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17ebb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Splitting data \n",
    "# split_factor = 1024\n",
    "# splitted_data = []\n",
    "# splitted_labels = []\n",
    "\n",
    "# global_batches = int(full_data.shape[0] / split_factor)\n",
    "# for i in range(global_batches):\n",
    "#     batch_data = full_data[i*split_factor: (i+1)*split_factor]\n",
    "#     batch_labels = full_labels[i*split_factor: (i+1)*split_factor]\n",
    "    \n",
    "#     splitted_labels.append(batch_labels)\n",
    "#     splitted_data.append(batch_data)\n",
    "    \n",
    "\n",
    "# splitted_data[-1] = np.concatenate((splitted_data[-1], full_data[len(splitted_data)*split_factor:]))\n",
    "# splitted_labels[-1] = np.concatenate((splitted_labels[-1], full_labels[len(splitted_labels)*split_factor:]))\n",
    "\n",
    "# data = splitted_data, splitted_labels\n",
    "\n",
    "split_factor = 1024\n",
    "splitted_data = []\n",
    "\n",
    "global_batches = int(full_data.shape[0] / split_factor)\n",
    "for i in range(global_batches):\n",
    "    batch_data = full_data[i*split_factor: (i+1)*split_factor]\n",
    "    splitted_data.append(batch_data)\n",
    "    \n",
    "splitted_data.append(full_data[len(splitted_data)*split_factor:])\n",
    "\n",
    "data = splitted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3181de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:33:06.538561: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6302880000 exceeds 10% of free system memory.\n",
      "2022-12-06 14:33:11.736379: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6302880000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#Loading validation data\n",
    "paths_val = np.concatenate((paths_bad_val, paths_good_val ), axis=0 )\n",
    "X_val, y_val = load_batch(paths_val)\n",
    "data_val = (X_val, y_val)\n",
    "data_val = tf.data.Dataset.from_tensor_slices(data_val).batch(32)#.prefetch(tf.data.AUTOTUNE)   # ToDo: also use .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97b523",
   "metadata": {},
   "source": [
    "### Creating model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0455249",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for resnet cnn fc training\n",
    "model_resnet = ResNet18(num_classes=5000)\n",
    "model_resnet.build((None,600,600,3))\n",
    "resnet_weights_path = '../models/ResNet/ResNet_original_border_600x600_best_94.59.hdf5'\n",
    "model_resnet.load_weights(resnet_weights_path)\n",
    "\n",
    "model_fc =  model = fc_model_softmax(input_num=5000)\n",
    "fc_weights_path = '../models/Softmax/FC_For_Resnet/best_custom_94_59_train.hdf5'\n",
    "model_fc.load_weights(fc_weights_path)\n",
    "\n",
    "model = resnet_fc(model_resnet, model_fc)\n",
    "model.build((None,600,600,3))\n",
    "weights_path = '../models/Softmax/ResNet_FC/resnet_fc_joint_5k_94_59_test.hdf5'\n",
    "model.save_weights(weights_path)\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38eb22f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet_fc_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " res_net18_1 (ResNet18)      multiple                  4074504   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 2)                 12616450  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,690,954\n",
      "Trainable params: 16,679,818\n",
      "Non-trainable params: 11,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbf203c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: \n",
      "Learnin rate: 0.003\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile()`: ({'options'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m lr_exp_decay(epoch, learning_rate)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearnin rate:\u001b[39m\u001b[38;5;124m'\u001b[39m, learning_rate)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSC_CE_KLD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-07\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(weights_path)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:3077\u001b[0m, in \u001b[0;36mModel._validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   3075\u001b[0m invalid_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(kwargs) \u001b[38;5;241m-\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_weight_mode\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m invalid_kwargs:\n\u001b[0;32m-> 3077\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid keyword argument(s) in `compile()`: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3078\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(invalid_kwargs,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Valid keyword arguments include \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3079\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloning\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperimental_run_tf_function\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribute\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3080\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3082\u001b[0m \u001b[38;5;66;03m# Model must be created and compiled with the same DistStrat.\u001b[39;00m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;129;01mand\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mhas_strategy():\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile()`: ({'options'},). Valid keyword arguments include \"cloning\", \"experimental_run_tf_function\", \"distribute\", \"target_tensors\", or \"sample_weight_mode\"."
     ]
    }
   ],
   "source": [
    "run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}: ')\n",
    "    learning_rate = lr_exp_decay(epoch, learning_rate)\n",
    "    print('Learnin rate:', learning_rate)\n",
    "    model.compile(loss=SC_CE_KLD,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=1e-07, decay=0, amsgrad=False), options = run_opts)\n",
    "    if epoch != 0:\n",
    "        model.load_weights(weights_path)\n",
    "        \n",
    "    random.shuffle(data)\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        verbose = 1\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "            verbose = 1\n",
    "            model.save_weights(weights_path)\n",
    "            \n",
    "        \n",
    "        batch_data = np.array(data[i])\n",
    "        random.shuffle(batch_data)\n",
    "\n",
    "#         k = time.time()\n",
    "        history = trainer(model, \n",
    "                          batch_data, \n",
    "                          data_val,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          learning_rate=learning_rate,\n",
    "                          verbose=verbose)    \n",
    "#         print('1 mini batch training: ', time.time() - k)\n",
    "\n",
    "#     model.save_weights(f'models/shufflenet_fc/Shufflenet_fc_21.09_512.h5', save_format='h5')\n",
    "    print('Done, epoch training!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34982f2",
   "metadata": {},
   "source": [
    "### Training on dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = tf.random.uniform((64, 600, 600, 3))\n",
    "labels = np.random.randint(0,1,(64, 16928))\n",
    "data = img_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = tf.random.uniform((4, 600, 600, 3))\n",
    "labels = np.random.randint(0,1,(4, 16928))\n",
    "data_val = img_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6295726",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.convert_to_tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer(model, \n",
    "                  data, \n",
    "                  weights_path,\n",
    "                  data_val,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs, \n",
    "                  learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da50bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
