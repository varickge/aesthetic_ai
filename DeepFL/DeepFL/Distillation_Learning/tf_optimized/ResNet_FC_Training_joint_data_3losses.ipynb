{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2baea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from final_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a776a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(gpus[1], True)\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#     try:\n",
    "#         tf.config.set_visible_devices(gpus[1], 'GPU')\n",
    "#         logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Visible devices must be set before GPUs have been initialized\n",
    "#         print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a68d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet_FC_joint import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1b60bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61d9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indxs = np.load('../../Genetic_Algorithm/best_res_all_res_996/best_solution_custom_95.72.npy')\n",
    "\n",
    "def take_from_vector(data, indxs=indxs):\n",
    "    new_data = np.squeeze(data[indxs])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac32fd",
   "metadata": {},
   "source": [
    "### DataLoader and other nesseccary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ba3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_exp_decay(epoch, lr):\n",
    "#     k = 0.04\n",
    "#     return lr * np.exp(-k*epoch)\n",
    "\n",
    "def make_dataset(paths, batch_size):\n",
    "    main_path = f'{root_path}Data/AesthAI/alm/splitted/alm_train/'\n",
    "    def parse_image(filename):\n",
    "        image = tf.io.read_file(filename)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        print(image)\n",
    "#         image = tf.image.resize(image, [996, 996]) #ToDo: check this, why we will resize img if we load alread resized img\n",
    "        return image\n",
    "\n",
    "    def configure_for_performance(ds):\n",
    "        ds = ds.shuffle(buffer_size=1000)\n",
    "        ds = ds.batch(batch_size)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return ds\n",
    "    for name in paths:\n",
    "        img_basename = os.path.basename(name).split('.')[0] + '.npy'\n",
    "        probs = np.load(root_path + f'Data/AesthAI/alm/splitted/alm_train/predictions/mg_cnn_fc/{img_basename}')\n",
    "        if 'good' in name:\n",
    "            label_img = 1\n",
    "        else:\n",
    "            label_img = 0\n",
    "        feat = np.concatenate((np.squeeze(np.load(main_path + f'features/multigap/all_res_996/' + img_basename)), \n",
    "                              np.squeeze(np.load(main_path + f'features/cnn_efficientnet_b7/border_600x600/' + img_basename))))[indxs]\n",
    "        labels = np.concatenate((feat, probs, np.array([label_img])))\n",
    "    filenames_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
    "    images_ds = filenames_ds.map(parse_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds = tf.data.Dataset.zip((images_ds, labels_ds))\n",
    "    ds = configure_for_performance(ds)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd814f3b",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ad2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, weights_path, train_paths, val_paths, batch_size=128, epochs=30, learning_rate=0.03, verbose=0):\n",
    "    data = make_dataset(train_paths, batch_size)\n",
    "    data_val = None#make_dataset(val_paths, batch_size)\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "   \n",
    "    history = model.fit(data, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=data_val, \n",
    "                        steps_per_epoch=math.ceil(len(train_paths)/32), validation_steps=math.ceil(len(train_paths)/32), \n",
    "                        validation_batch_size=batch_size, callbacks=callbacks_list)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894e5c7",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = f'{root_path}Data/AesthAI/alm/splitted/alm_train/'\n",
    "paths_bad = []\n",
    "paths_good = []\n",
    "    \n",
    "for i in range(7):\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    \n",
    "    for data in bad_data:\n",
    "        path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "        paths_bad.append(path_to_img)\n",
    "        \n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    path_to_img = main_path + f'images/{data[\"label\"]}/{data[\"splitted\"]}_resized_600x600/' + data['name']\n",
    "    paths_good.append(path_to_img)\n",
    "    \n",
    "for i in range(7):\n",
    "    paths_bad[i] = np.squeeze(np.array(paths_bad[i]))\n",
    "paths_good = np.squeeze(np.array(paths_good))\n",
    "   \n",
    "# Generating static validation data\n",
    "paths_bad, paths_bad_val = extract_static_val_data(paths_bad, perc = 0.014) #original - 0.017\n",
    "paths_good, paths_good_val = extract_static_val_data(paths_good, perc = 0.06) #original - 0.11\n",
    "\n",
    "paths_bad = np.array(paths_bad)\n",
    "paths_bad_val = np.array(paths_bad_val)\n",
    "paths_good = np.array(paths_good)\n",
    "paths_good_val = np.array(paths_good_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8842c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = np.concatenate((np.repeat(paths_good, 7), paths_bad))\n",
    "    \n",
    "#shuffling\n",
    "idx = np.random.permutation(len(full_data))\n",
    "full_data = full_data[idx]\n",
    "# full_data = full_data[:5000]  # debug\n",
    "full_data.shape\n",
    "paths_val = np.concatenate((paths_bad_val, paths_good_val ) , axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb97b523",
   "metadata": {},
   "source": [
    "### Creating model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0455249",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #64\n",
    "epochs = 20\n",
    "learning_rate = 0.009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53cbc7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResnetBlock.call of <ResNet.ResnetBlock object at 0x000001F883926E20>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResnetBlock.call of <ResNet.ResnetBlock object at 0x000001F883926E20>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"resnet_fc\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " res_net18_1 (ResNet18)      multiple                  4074504   \n",
      "                                                                 \n",
      " model_2 (Functional)        (None, 2)                 12616450  \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 2)                 5389058   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,080,012\n",
      "Trainable params: 9,456,522\n",
      "Non-trainable params: 12,623,490\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "model_res = ResNet18(num_classes=5000)\n",
    "model_fc = fc_model_softmax(input_num=5000)\n",
    "model = resnet_fc(model_res, model_fc)\n",
    "model.build((None, 996, 996, 3))\n",
    "\n",
    "# model.save_weights(weights_path)\n",
    "model.summary()\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=keras.optimizers.Adam(learning_rate=learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4995f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(996, 996, 3))\n",
    "output = resnet_fc()\n",
    "model = CustomModel(inputs, output(inputs))\n",
    "model.compile(optimizer=\"adam\")\n",
    "weights_path = '../models/Resnet/resnet_fc_996_3l/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcddd444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(None, None, 3), dtype=float32)\n",
      "Epoch 1/20\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Tensor(\"IteratorGetNext:0\", shape=(None, None, None, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\CareAware\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Distillation_Learning\\tf_optimized\\ResNet_FC_joint.py\", line 64, in train_step\n        feat_t = tf.slice(y, [0, 0], [32, 5000])\n\n    ValueError: Shape must be rank 2 but is rank 1 for '{{node Slice}} = Slice[Index=DT_INT32, T=DT_DOUBLE](IteratorGetNext:1, Slice/begin, Slice/size)' with input shapes: [?], [2], [2].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, weights_path, train_paths, val_paths, batch_size, epochs, learning_rate, verbose)\u001b[0m\n\u001b[0;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(weights_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      5\u001b[0m                                              mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m callbacks_list \u001b[38;5;241m=\u001b[39m [checkpoint]\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\CAREAW~1\\AppData\\Local\\Temp\\__autograph_generated_file9_24geh6.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Distillation_Learning\\tf_optimized\\ResNet_FC_joint.py:64\u001b[0m, in \u001b[0;36mCustomModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m---> 64\u001b[0m feat_t \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m probs_t \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mslice(y, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     66\u001b[0m label \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mslice(y, [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\CareAware\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\CareAware\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Distillation_Learning\\tf_optimized\\ResNet_FC_joint.py\", line 64, in train_step\n        feat_t = tf.slice(y, [0, 0], [32, 5000])\n\n    ValueError: Shape must be rank 2 but is rank 1 for '{{node Slice}} = Slice[Index=DT_INT32, T=DT_DOUBLE](IteratorGetNext:1, Slice/begin, Slice/size)' with input shapes: [?], [2], [2].\n"
     ]
    }
   ],
   "source": [
    "history = trainer(model, weights_path, full_data[:512], paths_val, batch_size=batch_size, epochs=epochs, learning_rate=learning_rate, \n",
    "                   verbose=1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53382e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.slice(y, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dde159",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_dataset(full_data[0], batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059c969",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388ca3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
