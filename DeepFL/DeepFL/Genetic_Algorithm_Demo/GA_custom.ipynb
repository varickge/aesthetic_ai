{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ecbc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad81b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "from final_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9775c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928b4c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = f'../Data/AesthAI/alm_96/splitted/alm_train/'\n",
    "bench_path = f'../Data/AesthAI/alm/splitted/alm_bench/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76de62",
   "metadata": {},
   "source": [
    "## Creating nessesary directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folder architecture in repository doesn't correspond to the demo\n",
    "if not os.path.exists(f'{main_path}features/multigap/original'):\n",
    "    os.makedirs(f'{main_path}features/multigap/original')\n",
    "if not os.path.exists(f'{main_path}features/cnn_efficientnet_b7/border_600x600'):\n",
    "    os.makedirs(f'{main_path}features/cnn_efficientnet_b7/border_600x600')\n",
    "if not os.path.exists(f'{bench_path}features/multigap/max_996'):\n",
    "    os.makedirs(f'{bench_path}features/multigap/max_996')\n",
    "if not os.path.exists(f'{bench_path}features/cnn_efficientnet_b7/border_600x600'):\n",
    "    os.makedirs(f'{bench_path}features/cnn_efficientnet_b7/border_600x600')\n",
    "    \n",
    "# The features extracted below will be put here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8baa236",
   "metadata": {},
   "source": [
    "## Creating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d63af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multigap  = model_inceptionresnet_multigap(model_path='../models/quality-mlsp-mtl-mse-loss.hdf5')\n",
    "model_cnn = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\", trainable=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d559f9c",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba088db",
   "metadata": {},
   "source": [
    "### Multigap (for train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0cd7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/good/good1\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad1\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad2\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad3\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad4\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad5\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad6\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train//images/bad/bad7\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train//features/multigap/original\n",
      "Extracted all...\n"
     ]
    }
   ],
   "source": [
    "source_file = f'{main_path}images/good/good1'\n",
    "target_file = f'{main_path}features/multigap/original'\n",
    "extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted='good1',\n",
    "                                 label='good',\n",
    "                                 resize_func=None,\n",
    "                                 size=None,\n",
    "                                 for_all=False,\n",
    "                                 model=model_multigap, \n",
    "                                 save_json=True)\n",
    "for i in range(7):\n",
    "    source_file = f'{main_path}/images/bad/bad{i+1}'\n",
    "    target_file = f'{main_path}/features/multigap/original'\n",
    "    extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted=f'bad{i+1}',\n",
    "                                 label='bad',\n",
    "                                 resize_func=None,\n",
    "                                 size = None,\n",
    "                                 for_all=False,\n",
    "                                 model=model_multigap, \n",
    "                                 save_json=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f99c54",
   "metadata": {},
   "source": [
    "### CNN (for train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd7cdde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/good/good1\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad1\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad2\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad3\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad4\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad5\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad6\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm_96/splitted/alm_train/images/bad/bad7\n",
      "Target =  ../Data/AesthAI/alm_96/splitted/alm_train/features/cnn_efficientnet_b7/border_600x600\n",
      "Extracted all...\n"
     ]
    }
   ],
   "source": [
    "source_file = f'{main_path}images/good/good1'\n",
    "target_file = f'{main_path}features/cnn_efficientnet_b7/border_600x600'\n",
    "extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted='good1',\n",
    "                                 label='good',\n",
    "                                 resize_func=resize_add_border,\n",
    "                                 size = (600, 600),\n",
    "                                 for_all=False,\n",
    "                                 model=model_cnn, \n",
    "                                 save_json=True)\n",
    "for i in range(7):\n",
    "    source_file = f'{main_path}images/bad/bad{i+1}'\n",
    "    target_file = f'{main_path}features/cnn_efficientnet_b7/border_600x600'\n",
    "    extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted=f'bad{i+1}',\n",
    "                                 label='bad',\n",
    "                                 resize_func=resize_add_border,\n",
    "                                 size = (600, 600),\n",
    "                                 for_all=False,\n",
    "                                 model=model_cnn, \n",
    "                                 save_json=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5331e",
   "metadata": {},
   "source": [
    "### Multigap (for benchmark data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153b99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source =  ../Data/AesthAI/alm/splitted/alm_bench/images/good/\n",
      "Target =  ../Data/AesthAI/alm/splitted/alm_bench/features/multigap/max_996\n",
      "100 images\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm/splitted/alm_bench/images/bad/\n",
      "Target =  ../Data/AesthAI/alm/splitted/alm_bench/features/multigap/max_996\n",
      "100 images\n",
      "Extracted all...\n"
     ]
    }
   ],
   "source": [
    "source_file = f'{bench_path}images/good/'\n",
    "target_file = f'{bench_path}features/multigap/max_996'\n",
    "extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted='good',\n",
    "                                 label='good',\n",
    "                                 resize_func=resize_max,\n",
    "                                 size = (996, 996),\n",
    "                                 for_all=False,\n",
    "                                 model=model_multigap, \n",
    "                                 save_json=True)\n",
    "\n",
    "source_file = f'{bench_path}images/bad/'\n",
    "target_file = f'{bench_path}features/multigap/max_996'\n",
    "extract_features_from_path_automated_json(\n",
    "                             source_file=source_file,\n",
    "                             target_file=target_file,\n",
    "                             splitted='bad',\n",
    "                             label='bad',\n",
    "                             resize_func=resize_max,\n",
    "                             size = (996, 996),\n",
    "                             for_all=False,\n",
    "                             model=model_multigap, \n",
    "                             save_json=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c4fec5",
   "metadata": {},
   "source": [
    "### CNN (for benchmark data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edbf4aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source =  ../Data/AesthAI/alm/splitted/alm_bench/images/good/\n",
      "Target =  ../Data/AesthAI/alm/splitted/alm_bench/features/cnn_efficientnet_b7/border_600x600\n",
      "100 images\n",
      "Extracted all...\n",
      "Source =  ../Data/AesthAI/alm/splitted/alm_bench//images/bad/\n",
      "Target =  ../Data/AesthAI/alm/splitted/alm_bench//features/cnn_efficientnet_b7/border_600x600\n",
      "100 images\n",
      "Extracted all...\n"
     ]
    }
   ],
   "source": [
    "source_file = f'{bench_path}images/good/'\n",
    "target_file = f'{bench_path}features/cnn_efficientnet_b7/border_600x600'\n",
    "extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted='good',\n",
    "                                 label='good',\n",
    "                                 resize_func=resize_add_border,\n",
    "                                 size = (600, 600),\n",
    "                                 for_all=False,\n",
    "                                 model=model_cnn, \n",
    "                                 save_json=True)\n",
    "\n",
    "source_file = f'{bench_path}/images/bad/'\n",
    "target_file = f'{bench_path}/features/cnn_efficientnet_b7/border_600x600'\n",
    "extract_features_from_path_automated_json(\n",
    "                             source_file=source_file,\n",
    "                             target_file=target_file,\n",
    "                             splitted=f'bad',\n",
    "                             label='bad',\n",
    "                             resize_func=resize_add_border,\n",
    "                             size = (600, 600),\n",
    "                             for_all=False,\n",
    "                             model=model_cnn, \n",
    "                             save_json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce29238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main function which executes Genetic Algorithm. For more details check utils.py\n",
    "\n",
    "def findBestFeats(pop_size, feature_vector_size, transformed_feat_vector_size, num_of_parents=4, max_gen=100, init_size=2,\n",
    "                  weights_path =  f'models/custom_ga_popsize_100_vectorsize_5000.hdf5',\n",
    "                  train_data_path=main_path,\n",
    "                  bench_path=bench_path,\n",
    "                  feats_MG='max_996',\n",
    "                  feats_CNN='border_600x600', cnn='cnn_efficientnet_b7'):\n",
    "    # Creating FC model\n",
    "    model = fc_model_softmax(input_num=transformed_feat_vector_size)\n",
    "    # Loading train data\n",
    "    data, lbl, val_data, val_lbl = loading_data_from_json(main_path=train_data_path,\n",
    "                                                                    feats_CNN=feats_CNN,\n",
    "                                                                    cnn=cnn)\n",
    "    \n",
    "    # loading benchmark data \n",
    "    bench, bench_labels = loading_bench_data(bench_path=bench_path,\n",
    "                                             feats_MG=feats_MG,\n",
    "                                             feats_CNN=feats_CNN,\n",
    "                                             cnn=cnn)\n",
    "    \n",
    "    print(\"Generating random population\")\n",
    "    # Initializing population for first generation\n",
    "    pop = generate_init_pop(feature_vector_size, transformed_feat_vector_size, pop_size, init_size)\n",
    "\n",
    "    gen = 1\n",
    "    while gen <= max_gen:\n",
    "        pop_acc = [] \n",
    "        for i in range(len(pop)):\n",
    "            # Cycle_for_population is used to train, evaluate and return accuracy for each given solution \n",
    "            acc = cycle_for_population(curr_pop=pop[i],\n",
    "                                       weights_path=weights_path,\n",
    "                                       model=model, \n",
    "                                       data=data,\n",
    "                                       val_data=val_data,\n",
    "                                       lbl=lbl,\n",
    "                                       val_lbl=val_lbl,\n",
    "                                       bench=bench,\n",
    "                                       bench_labels=bench_labels,\n",
    "                                       transformed_feat_vector_size=transformed_feat_vector_size,\n",
    "                                       epochs=1,\n",
    "                                       learning_rate = 0.003,\n",
    "                                       batch_size = 128)\n",
    "            \n",
    "            pop_acc = np.concatenate((pop_acc, acc[None]))\n",
    "\n",
    "        print('Selection')\n",
    "        # Selects solutions for crossover, mutation\n",
    "        idxs_for_cross, idxs_for_mut = selection(pop_acc, num_of_parents=num_of_parents)\n",
    "        \n",
    "        parents = pop[idxs_for_cross]\n",
    "        parents_for_mut = pop[idxs_for_mut]\n",
    "\n",
    "        print('Crossover')\n",
    "        children = crossover(parents)\n",
    "        \n",
    "        print('Mutation')\n",
    "        pop[idxs_for_mut] = mutation(parents_for_mut)\n",
    "        # Creates new population for next generation using children and mutated solutions\n",
    "        pop = new_population(pop, pop_acc, children)\n",
    "\n",
    "        print(f'Generation {gen}')\n",
    "        gen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68761432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random population\n",
      "0\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.3934\n",
      "Epoch 1: val_loss improved from inf to 0.00680, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 3s 17ms/step - loss: 0.3856 - val_loss: 0.0068\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0157\n",
      "Epoch 1: val_loss improved from inf to 0.00860, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0153 - val_loss: 0.0086\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0104\n",
      "Epoch 1: val_loss improved from inf to 0.00903, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0103 - val_loss: 0.0090\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0088\n",
      "Epoch 1: val_loss improved from inf to 0.00697, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0091 - val_loss: 0.0070\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0073\n",
      "Epoch 1: val_loss improved from inf to 0.00829, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0073 - val_loss: 0.0083\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0068\n",
      "Epoch 1: val_loss improved from inf to 0.01098, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0068 - val_loss: 0.0110\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.0058\n",
      "Epoch 1: val_loss improved from inf to 0.01081, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 16ms/step - loss: 0.0057 - val_loss: 0.0108\n",
      "ACC: 50.0\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3857\n",
      "Epoch 1: val_loss improved from inf to 0.00702, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 0.3828 - val_loss: 0.0070\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0177\n",
      "Epoch 1: val_loss improved from inf to 0.00892, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 14ms/step - loss: 0.0175 - val_loss: 0.0089\n",
      "74/79 [===========================>..] - ETA: 0s - loss: 0.0088\n",
      "Epoch 1: val_loss improved from inf to 0.01322, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0084 - val_loss: 0.0132\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0068\n",
      "Epoch 1: val_loss improved from inf to 0.00727, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0066 - val_loss: 0.0073\n",
      "75/79 [===========================>..] - ETA: 0s - loss: 0.0046\n",
      "Epoch 1: val_loss improved from inf to 0.01164, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0044 - val_loss: 0.0116\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0079\n",
      "Epoch 1: val_loss improved from inf to 0.01300, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0079 - val_loss: 0.0130\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.0084\n",
      "Epoch 1: val_loss improved from inf to 0.00612, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 16ms/step - loss: 0.0084 - val_loss: 0.0061\n",
      "ACC: 50.0\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4114\n",
      "Epoch 1: val_loss improved from inf to 0.01444, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 0.4114 - val_loss: 0.0144\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0255\n",
      "Epoch 1: val_loss improved from inf to 0.00736, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0255 - val_loss: 0.0074\n",
      "74/79 [===========================>..] - ETA: 0s - loss: 0.0088\n",
      "Epoch 1: val_loss improved from inf to 0.00940, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0084 - val_loss: 0.0094\n",
      "74/79 [===========================>..] - ETA: 0s - loss: 0.0087\n",
      "Epoch 1: val_loss improved from inf to 0.00627, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0082 - val_loss: 0.0063\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0059\n",
      "Epoch 1: val_loss improved from inf to 0.01503, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0058 - val_loss: 0.0150\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0050\n",
      "Epoch 1: val_loss improved from inf to 0.01405, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0050 - val_loss: 0.0141\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.0077\n",
      "Epoch 1: val_loss improved from inf to 0.00934, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 16ms/step - loss: 0.0077 - val_loss: 0.0093\n",
      "ACC: 53.0\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3734\n",
      "Epoch 1: val_loss improved from inf to 0.00626, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 18ms/step - loss: 0.3734 - val_loss: 0.0063\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0165\n",
      "Epoch 1: val_loss improved from inf to 0.00772, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0165 - val_loss: 0.0077\n",
      "73/79 [==========================>...] - ETA: 0s - loss: 0.0133\n",
      "Epoch 1: val_loss improved from inf to 0.00809, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0134 - val_loss: 0.0081\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0101\n",
      "Epoch 1: val_loss improved from inf to 0.00897, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0101 - val_loss: 0.0090\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0116\n",
      "Epoch 1: val_loss improved from inf to 0.01087, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0114 - val_loss: 0.0109\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0037\n",
      "Epoch 1: val_loss improved from inf to 0.01564, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0037 - val_loss: 0.0156\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 1: val_loss improved from inf to 0.00881, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 16ms/step - loss: 0.0033 - val_loss: 0.0088\n",
      "ACC: 51.0\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4067\n",
      "Epoch 1: val_loss improved from inf to 0.01060, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 16ms/step - loss: 0.3991 - val_loss: 0.0106\n",
      "75/79 [===========================>..] - ETA: 0s - loss: 0.0186\n",
      "Epoch 1: val_loss improved from inf to 0.01029, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0179 - val_loss: 0.0103\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0095\n",
      "Epoch 1: val_loss improved from inf to 0.01139, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0092 - val_loss: 0.0114\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0060\n",
      "Epoch 1: val_loss improved from inf to 0.01099, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0060 - val_loss: 0.0110\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0067\n",
      "Epoch 1: val_loss improved from inf to 0.00815, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0067 - val_loss: 0.0081\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0048\n",
      "Epoch 1: val_loss improved from inf to 0.01019, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0048 - val_loss: 0.0102\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.0032\n",
      "Epoch 1: val_loss improved from inf to 0.01182, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 16ms/step - loss: 0.0031 - val_loss: 0.0118\n",
      "ACC: 50.0\n",
      "68/70 [============================>.] - ETA: 0s - loss: 0.4048\n",
      "Epoch 1: val_loss improved from inf to 0.00710, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 0.3965 - val_loss: 0.0071\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0179\n",
      "Epoch 1: val_loss improved from inf to 0.00532, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0178 - val_loss: 0.0053\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0087\n",
      "Epoch 1: val_loss improved from inf to 0.01176, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0095 - val_loss: 0.0118\n",
      "78/79 [============================>.] - ETA: 0s - loss: 0.0075\n",
      "Epoch 1: val_loss improved from inf to 0.01082, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0074 - val_loss: 0.0108\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0062\n",
      "Epoch 1: val_loss improved from inf to 0.01017, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0062 - val_loss: 0.0102\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 1: val_loss improved from inf to 0.01401, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0042 - val_loss: 0.0140\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.0057\n",
      "Epoch 1: val_loss improved from inf to 0.00459, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 17ms/step - loss: 0.0057 - val_loss: 0.0046\n",
      "ACC: 50.0\n",
      "Selection\n",
      "Crossover\n",
      "Mutation\n",
      "Generation 1\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3883\n",
      "Epoch 1: val_loss improved from inf to 0.00820, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 0.3857 - val_loss: 0.0082\n",
      "76/79 [===========================>..] - ETA: 0s - loss: 0.0189\n",
      "Epoch 1: val_loss improved from inf to 0.00637, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0185 - val_loss: 0.0064\n",
      "75/79 [===========================>..] - ETA: 0s - loss: 0.0114\n",
      "Epoch 1: val_loss improved from inf to 0.00743, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0110 - val_loss: 0.0074\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0091\n",
      "Epoch 1: val_loss improved from inf to 0.01020, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 17ms/step - loss: 0.0096 - val_loss: 0.0102\n",
      "75/79 [===========================>..] - ETA: 0s - loss: 0.0092\n",
      "Epoch 1: val_loss improved from inf to 0.00789, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0110 - val_loss: 0.0079\n",
      "73/79 [==========================>...] - ETA: 0s - loss: 0.0086\n",
      "Epoch 1: val_loss improved from inf to 0.00708, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 15ms/step - loss: 0.0085 - val_loss: 0.0071\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.0079\n",
      "Epoch 1: val_loss improved from inf to 0.00239, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "84/84 [==============================] - 2s 15ms/step - loss: 0.0076 - val_loss: 0.0024\n",
      "ACC: 51.0\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3977\n",
      "Epoch 1: val_loss improved from inf to 0.00798, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "70/70 [==============================] - 2s 17ms/step - loss: 0.3950 - val_loss: 0.0080\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.0193\n",
      "Epoch 1: val_loss improved from inf to 0.00676, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0193 - val_loss: 0.0068\n",
      "74/79 [===========================>..] - ETA: 0s - loss: 0.0103\n",
      "Epoch 1: val_loss improved from inf to 0.01266, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0098 - val_loss: 0.0127\n",
      "77/79 [============================>.] - ETA: 0s - loss: 0.0105\n",
      "Epoch 1: val_loss improved from inf to 0.01005, saving model to models\\custom_ga_popsize_100_vectorsize_5000.hdf5\n",
      "79/79 [==============================] - 2s 16ms/step - loss: 0.0110 - val_loss: 0.0101\n",
      "11/79 [===>..........................] - ETA: 1s - loss: 0.0014"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfindBestFeats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_vector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m19488\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_feat_vector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_of_parents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m              \u001b[49m\u001b[43minit_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mfindBestFeats\u001b[1;34m(pop_size, feature_vector_size, transformed_feat_vector_size, num_of_parents, max_gen, init_size, weights_path, train_data_path, bench_path, feats_MG, feats_CNN, cnn)\u001b[0m\n\u001b[0;32m     28\u001b[0m pop_acc \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pop)):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Cycle_for_population is used to train, evaluate and return accuracy for each given solution \u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[43mcycle_for_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_pop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlbl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlbl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mval_lbl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_lbl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbench\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbench_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbench_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtransformed_feat_vector_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformed_feat_vector_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     pop_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((pop_acc, acc[\u001b[38;5;28;01mNone\u001b[39;00m]))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Genetic_Algorithm_Demo\\utils.py:325\u001b[0m, in \u001b[0;36mcycle_for_population\u001b[1;34m(curr_pop, weights_path, model, data, val_data, lbl, val_lbl, bench, bench_labels, transformed_feat_vector_size, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m    323\u001b[0m     val_data_transformed \u001b[38;5;241m=\u001b[39m take_from_feats(val_data, curr_pop)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;66;03m#Model training\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbl\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_data_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_lbl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m#Model evaluation\u001b[39;00m\n\u001b[0;32m    334\u001b[0m acc \u001b[38;5;241m=\u001b[39m predict(bench, bench_labels, model, curr_pop)\n",
      "File \u001b[1;32m~\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Genetic_Algorithm_Demo\\utils.py:218\u001b[0m, in \u001b[0;36mtrainer\u001b[1;34m(model, data, weights_path, data_val, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m#     schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     callbacks_list \u001b[38;5;241m=\u001b[39m [checkpoint] \u001b[38;5;66;03m# schedule, add for lr decay\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:1413\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1411\u001b[0m   context\u001b[38;5;241m.\u001b[39masync_wait()\n\u001b[0;32m   1412\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m \u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_increment\u001b[49m\n\u001b[0;32m   1414\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1268\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m steps_remaining\n\u001b[0;32m   1266\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution\u001b[38;5;241m.\u001b[39massign(original_spe)\n\u001b[1;32m-> 1268\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_increment\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1270\u001b[0m   \u001b[38;5;124;03m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "findBestFeats(pop_size=6, feature_vector_size=19488, transformed_feat_vector_size=5000, num_of_parents=3, max_gen=100, \n",
    "              init_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527ec91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccf0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
