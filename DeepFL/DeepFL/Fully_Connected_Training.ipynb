{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b4cd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *\n",
    "\n",
    "# %load_ext tensorboard\n",
    "random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c4c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220f0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_CE_KLD = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddf9f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = generate_root_path() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b0d80",
   "metadata": {},
   "source": [
    "### Loading alm data features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a282a08",
   "metadata": {},
   "source": [
    "#### Loading  multigap features from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_path = root_path + 'Data/AesthAI/alm/splitted/alm_train/'\n",
    "# features_bad_list = []\n",
    "# features_bad_list_i = []\n",
    "# features_good1 = []\n",
    "# feats = 'original_PCA_8464_auto'\n",
    "    \n",
    "# for i in range(7):\n",
    "#     alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "#     bad_data = json.load(alm_train_bad)\n",
    "#     for data in bad_data:\n",
    "#         feat_path = main_path + f'features/multigap/{feats}/' + data['feature']\n",
    "#         features_bad_list_i.append(np.load(feat_path))\n",
    "#     features_bad_list.append(features_bad_list_i)\n",
    "#     features_bad_list_i = []\n",
    "        \n",
    "# alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "# good_data = json.load(alm_train_good)\n",
    "# for data in good_data:\n",
    "#     feat_path = main_path + f'features/multigap/{feats}/' + data['feature']\n",
    "#     features_good1.append(np.load(feat_path))\n",
    "    \n",
    "# for i in range(7):\n",
    "#     features_bad_list[i] = np.squeeze(np.array(features_bad_list[i]))\n",
    "# features_good1 = np.squeeze(np.array(features_good1))\n",
    "   \n",
    "# # Generating static validation data\n",
    "# features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "# features_good1, features_good1_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "\n",
    "# bad = features_bad_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e9f2a",
   "metadata": {},
   "source": [
    "#### Loading cnn features + multigap features from .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6e14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    7    50    62 ...  3282  6701 11051]\n"
     ]
    }
   ],
   "source": [
    "# Loading pca models in case we want to use pca\n",
    "\n",
    "# pca_mg_path = 'models/PCA/PCA_mg_171.pkl'\n",
    "# pca_mg = pk.load(open(pca_mg_path,'rb'))\n",
    "\n",
    "# pca_cnn_path = 'models/PCA/PCA_CNN_2050.pkl'\n",
    "# pca_cnn = pk.load(open(pca_cnn_path,'rb'))\n",
    "\n",
    "# Loading take solution and defining take function\n",
    "indxs = np.load('Genetic_Algorithm/best_res/BEST_max_996.npy')\n",
    "def take_from_vector(data, indxs=indxs):\n",
    "    new_data = np.squeeze(data[indxs])\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f24d2d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         feat_path_2 \u001b[38;5;241m=\u001b[39m root_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Data/AesthAI/alm/splitted/alm_train/features/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeats_CNN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     17\u001b[0m         feat_1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(feat_path_1)\n\u001b[1;32m---> 18\u001b[0m         feat_2 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_path_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#        feat_1 = pca_mg.transform(feat_1)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#        feat_2 = pca_cnn.transform(feat_2)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         connected \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39msqueeze(feat_1), np\u001b[38;5;241m.\u001b[39msqueeze(feat_2)))\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\numpy\\lib\\npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    408\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_path = root_path + 'Data/AesthAI/alm/splitted/alm_train/' \n",
    "features_bad_list = []\n",
    "features_bad_list_i = []\n",
    "features_good1 = []\n",
    "feats_MG = 'original'  \n",
    "feats_CNN = 'border_600x600'\n",
    "cnn = 'cnn_efficientnet_b7'\n",
    "    \n",
    "for i in range(7):\n",
    "    # Loading train bad data\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    for data in bad_data:\n",
    "        feat_path_1 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/multigap/{feats_MG}/\" + data['feature']\n",
    "        feat_path_2 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/{cnn}/{feats_CNN}/\" + data['feature']\n",
    "\n",
    "        feat_1 = np.load(feat_path_1)\n",
    "        feat_2 = np.load(feat_path_2)\n",
    "        \n",
    "#        feat_1 = pca_mg.transform(feat_1)\n",
    "#        feat_2 = pca_cnn.transform(feat_2)\n",
    "\n",
    "        connected = np.concatenate((np.squeeze(feat_1), np.squeeze(feat_2)))\n",
    "        connected = take_from_vector(np.squeeze(connected)) # must be commented later\n",
    "        \n",
    "        features_bad_list_i.append(connected)\n",
    "        \n",
    "    features_bad_list.append(features_bad_list_i)\n",
    "    features_bad_list_i = []\n",
    "    \n",
    "# Loading train good data\n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    feat_path_1 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/multigap/{feats_MG}/\" + data['feature']\n",
    "    feat_path_2 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/{cnn}/{feats_CNN}/\" + data['feature']\n",
    " \n",
    "    feat_1 = np.load(feat_path_1)\n",
    "    feat_2 = np.load(feat_path_2)\n",
    "    \n",
    "#    feat_1 = pca_mg.transform(feat_1)\n",
    "#    feat_2 = pca_mg.transform(feat_2)\n",
    "    \n",
    "    connected = np.concatenate((np.squeeze(feat_1), np.squeeze(feat_2)))\n",
    "    connected = take_from_vector(np.squeeze(connected)) # must be commented later\n",
    "    \n",
    "    features_good1.append(connected)\n",
    "    \n",
    "for i in range(7):\n",
    "    features_bad_list[i] = np.squeeze(np.array(features_bad_list[i]))\n",
    "features_good1 = np.squeeze(np.array(features_good1))\n",
    "   \n",
    "# Generating static validation data\n",
    "features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "features_good1, features_good1_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "\n",
    "X_val = np.concatenate( (features_bad1_val, features_good1_val ) , axis=0 )\n",
    "y_val = np.concatenate( (np.zeros(len(features_bad1_val)), np.ones(len(features_good1_val)) ), axis=0 )\n",
    "\n",
    "bad = features_bad_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8c9d3",
   "metadata": {},
   "source": [
    "### Loading resnet + cnn features from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26f03e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = root_path + 'Data/AesthAI/alm/splitted/alm_train/' \n",
    "features_bad_list = []\n",
    "features_bad_list_i = []\n",
    "features_good1 = []\n",
    "feats_res = 'border_600x600_5k_94_24'  \n",
    "feats_CNN = 'border_600x600'\n",
    "cnn = 'cnn_efficientnet_b7'\n",
    "    \n",
    "for i in range(7):\n",
    "    # Loading train bad data\n",
    "    alm_train_bad = open(f'{main_path}data_bad{i+1}.json')\n",
    "    bad_data = json.load(alm_train_bad)\n",
    "    for data in bad_data:\n",
    "        feat_path_1 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/resnet/{feats_res}/\" + data['feature']\n",
    "#         feat_path_2 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/{cnn}/{feats_CNN}/\" + data['feature']\n",
    "\n",
    "        feat_1 = np.squeeze(np.load(feat_path_1))\n",
    "#         feat_2 = np.load(feat_path_2)\n",
    "#         feat_2 = take_from_vector(np.squeeze(feat_2)) # must be commented later\n",
    "        \n",
    "\n",
    "#         connected = np.concatenate((np.squeeze(feat_1), np.squeeze(feat_2)))\n",
    "        \n",
    "        features_bad_list_i.append(feat_1)\n",
    "        \n",
    "    features_bad_list.append(features_bad_list_i)\n",
    "    features_bad_list_i = []\n",
    "    \n",
    "# Loading train good data\n",
    "alm_train_good = open(f'{main_path}/data_good1.json')\n",
    "good_data = json.load(alm_train_good)\n",
    "for data in good_data:\n",
    "    feat_path_1 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/resnet/{feats_res}/\" + data['feature']\n",
    "#         feat_path_2 = root_path + f\"/Data/AesthAI/alm/splitted/alm_train/features/{cnn}/{feats_CNN}/\" + data['feature']\n",
    "\n",
    "    feat_1 = np.squeeze(np.load(feat_path_1))\n",
    "#         feat_2 = np.load(feat_path_2)\n",
    "#         feat_2 = take_from_vector(np.squeeze(feat_2)) # must be commented later\n",
    "        \n",
    "\n",
    "#         connected = np.concatenate((np.squeeze(feat_1), np.squeeze(feat_2)))\n",
    "    \n",
    "    features_good1.append(feat_1)\n",
    "    \n",
    "for i in range(7):\n",
    "    features_bad_list[i] = np.squeeze(np.array(features_bad_list[i]))\n",
    "features_good1 = np.squeeze(np.array(features_good1))\n",
    "   \n",
    "# Generating static validation data\n",
    "features_bad_list[0], features_bad1_val = extract_static_val_data(features_bad_list[0], perc = 0.11)\n",
    "features_good1, features_good1_val = extract_static_val_data(features_good1, perc = 0.11)\n",
    "\n",
    "X_val = np.concatenate( (features_bad1_val, features_good1_val ) , axis=0 )\n",
    "y_val = np.concatenate( (np.zeros(len(features_bad1_val)), np.ones(len(features_good1_val)) ), axis=0 )\n",
    "\n",
    "bad = features_bad_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45acec56",
   "metadata": {},
   "source": [
    "### Some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7247a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_graphs(history):\n",
    "    plt.plot(np.arange(2, len(history['loss'])+1), history['loss'][1:])\n",
    "    plt.plot(np.arange(2, len(history['loss'])+1), history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def save_history(history, name):\n",
    "    np.savez(name, val_loss=np.array(history.history['val_loss']),\n",
    "                       loss=np.array(history.history['loss']))\n",
    "    \n",
    "    \n",
    "def calc_acc(model, weights_path, X_test, y_test, batch_size):\n",
    "    '''\n",
    "    Compares Max classes with targets, getting mean class precision\n",
    "    '''\n",
    "    model.load_weights(weights_path)\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    acc = (np.argmax(y_pred,axis=-1) == y_test).mean()\n",
    "    return acc\n",
    "\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.048\n",
    "    return lr * np.exp(-k*epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233bbc9",
   "metadata": {},
   "source": [
    "### Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44582a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainer(model, data, weights_path, data_val,batch_size=128, epochs=10, learning_rate=0.03,\n",
    "            model_name = None, index = None):\n",
    "    X_train, X_test, y_train, y_test = data\n",
    "\n",
    "    model.compile(loss=SC_CE_KLD,\n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, \n",
    "                                                  epsilon=1e-07, decay=0, amsgrad=False))\n",
    "    model.load_weights(weights_path) \n",
    "\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(weights_path, \n",
    "                                                 monitor='val_loss', \n",
    "                                                 verbose=1, \n",
    "                                                 save_best_only=True, \n",
    "                                                 mode='min')\n",
    "    \n",
    "    \n",
    "    schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\n",
    "#     log_dir = f'logs/fit/{model_name}/{index}'\n",
    "#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    callbacks_list = [checkpoint, schedule] #, tensorboard_callback\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data = data_val)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f10ae",
   "metadata": {},
   "source": [
    "# Experiments with FC hidden layers sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443d1bf",
   "metadata": {},
   "source": [
    "### Creating and loading weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85a38391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_92_24_5k_2'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = fc_model_softmax(input_num=5000)\n",
    "weights_path_pretrained = f'models/FC_resnet/best_92_24_5k_2.hdf5'\n",
    "model.save_weights(weights_path_pretrained) #if we want to cancel learning and start from 0\n",
    "model.load_weights(weights_path_pretrained)\n",
    "weights_path = f'models/FC_resnet/best_92_24_5k_2.hdf5'\n",
    "# model.save_weights(weights_path)\n",
    "model_name = os.path.basename(weights_path).split('.')[0]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d42d9a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e05635a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c0293f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8899, 5000)\n",
      "(10000, 5000)\n",
      "(10000, 5000)\n",
      "(9999, 5000)\n",
      "(9998, 5000)\n",
      "(9997, 5000)\n",
      "(10717, 5000)\n",
      "(10465, 5000)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bad)):\n",
    "    print(bad[i].shape)\n",
    "print(features_good1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d68ecc69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_bad shape (10717, 5000)\n",
      "train_data_good shape (10465, 5000)\n",
      "target_data shape (21182,)\n",
      "Epoch 1/15\n",
      "153/158 [============================>.] - ETA: 0s - loss: 0.3370\n",
      "Epoch 1: val_loss improved from inf to 0.34882, saving model to models/FC_resnet\\best_92_24_5k_2.hdf5\n",
      "158/158 [==============================] - 2s 12ms/step - loss: 0.3361 - val_loss: 0.3488 - lr: 0.0030\n",
      "Epoch 2/15\n",
      "158/158 [==============================] - ETA: 0s - loss: 0.3290\n",
      "Epoch 2: val_loss did not improve from 0.34882\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.3290 - val_loss: 0.3523 - lr: 0.0029\n",
      "Epoch 3/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.3242\n",
      "Epoch 3: val_loss did not improve from 0.34882\n",
      "158/158 [==============================] - 1s 9ms/step - loss: 0.3242 - val_loss: 0.3539 - lr: 0.0026\n",
      "Epoch 4/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.3172\n",
      "Epoch 4: val_loss did not improve from 0.34882\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.3175 - val_loss: 0.4210 - lr: 0.0022\n",
      "Epoch 5/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.3156\n",
      "Epoch 5: val_loss did not improve from 0.34882\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.3157 - val_loss: 0.3519 - lr: 0.0019\n",
      "Epoch 6/15\n",
      "155/158 [============================>.] - ETA: 0s - loss: 0.3110\n",
      "Epoch 6: val_loss did not improve from 0.34882\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.3110 - val_loss: 0.3649 - lr: 0.0015\n",
      "Epoch 7/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.3027\n",
      "Epoch 7: val_loss improved from 0.34882 to 0.33631, saving model to models/FC_resnet\\best_92_24_5k_2.hdf5\n",
      "158/158 [==============================] - 2s 11ms/step - loss: 0.3021 - val_loss: 0.3363 - lr: 0.0011\n",
      "Epoch 8/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.2970\n",
      "Epoch 8: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2979 - val_loss: 0.3369 - lr: 7.8240e-04\n",
      "Epoch 9/15\n",
      "153/158 [============================>.] - ETA: 0s - loss: 0.2966\n",
      "Epoch 9: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2972 - val_loss: 0.3469 - lr: 5.3292e-04\n",
      "Epoch 10/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.2874\n",
      "Epoch 10: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2873 - val_loss: 0.3444 - lr: 3.4598e-04\n",
      "Epoch 11/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.2881\n",
      "Epoch 11: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2875 - val_loss: 0.3389 - lr: 2.1408e-04\n",
      "Epoch 12/15\n",
      "153/158 [============================>.] - ETA: 0s - loss: 0.2848\n",
      "Epoch 12: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2845 - val_loss: 0.3432 - lr: 1.2626e-04\n",
      "Epoch 13/15\n",
      "154/158 [============================>.] - ETA: 0s - loss: 0.2845\n",
      "Epoch 13: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2837 - val_loss: 0.3427 - lr: 7.0978e-05\n",
      "Epoch 14/15\n",
      "156/158 [============================>.] - ETA: 0s - loss: 0.2850\n",
      "Epoch 14: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2849 - val_loss: 0.3440 - lr: 3.8030e-05\n",
      "Epoch 15/15\n",
      "157/158 [============================>.] - ETA: 0s - loss: 0.2834\n",
      "Epoch 15: val_loss did not improve from 0.33631\n",
      "158/158 [==============================] - 2s 10ms/step - loss: 0.2835 - val_loss: 0.3405 - lr: 1.9421e-05\n",
      "----- Accuracy = 0.8620401337792643  -----\n",
      "---Batch Train Done---\n"
     ]
    }
   ],
   "source": [
    "data_val = (X_val, y_val)\n",
    "\n",
    "i = 6\n",
    "data = get_train_pairs(bad[i], features_good1, train_size=0.95, shuffle=True)\n",
    "if True:\n",
    "    history = trainer(model, data, weights_path, data_val, batch_size, epochs, \n",
    "                      learning_rate=learning_rate, model_name=model_name, index=i)\n",
    "    acc = calc_acc(model, weights_path, data_val[0], data_val[1], batch_size)\n",
    "\n",
    "    print('----- Accuracy =', acc, ' -----')\n",
    "    print('---Batch Train Done---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit/GA_test_5000_best --host 192.168.5.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9531e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
