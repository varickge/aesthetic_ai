{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874db585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CareAware\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\\Distillation_Learning\n"
     ]
    }
   ],
   "source": [
    "cd Distillation_Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a632935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResNet import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88659e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CareAware\\Consulting_Projects\\aesthetic-ai\\DeepFL\\DeepFL\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb55a2",
   "metadata": {},
   "source": [
    "### Creating model and generating root path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74d4328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method ResnetBlock.call of <ResNet.ResnetBlock object at 0x0000022A149B3160>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ResnetBlock.call of <ResNet.ResnetBlock object at 0x0000022A149B3160>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "root_path = generate_root_path() \n",
    "\n",
    "# model = model_inceptionresnet_multigap()\n",
    "# model = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\", trainable=False)])\n",
    "model_resnet = ResNet18(num_classes=5000)\n",
    "model_resnet.build((None, 600, 600, 3))\n",
    "weights_path = 'models/ResNet/ResNet_original_border_600x600_best_94.24.hdf5'\n",
    "model_resnet.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6d2c5",
   "metadata": {},
   "source": [
    "### Feature extracting from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc25783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# source_file = root_path + 'Data/AesthAI/benchmark2/images/bad'\n",
    "# target_file = root_path + 'Data/AesthAI/benchmark2/features/multigap/original/'\n",
    "\n",
    "# extract_features_from_path_automated_json(source_file=source_file, target_file=target_file, model=model, \n",
    "#                                           label='bad', crop_func=None, resize_func=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130549a8",
   "metadata": {},
   "source": [
    "### Feature extracting from path and saving to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "890a3794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source =  D:/Data/AesthAI/alm/splitted/alm_train/images/bad/bad7/\n",
      "Target =  D:/Data/AesthAI/alm/splitted/alm_train/features/resnet/border_600x600_5k_94_24/\n",
      "100 images\n",
      "200 images\n",
      "300 images\n",
      "400 images\n",
      "500 images\n",
      "600 images\n",
      "700 images\n",
      "800 images\n",
      "900 images\n",
      "1000 images\n",
      "1100 images\n",
      "1200 images\n",
      "1300 images\n",
      "1400 images\n",
      "1500 images\n",
      "1600 images\n",
      "Skip\n",
      "1700 images\n",
      "1800 images\n",
      "1900 images\n",
      "2000 images\n",
      "2100 images\n",
      "2200 images\n",
      "2300 images\n",
      "2400 images\n",
      "2500 images\n",
      "2600 images\n",
      "2700 images\n",
      "2800 images\n",
      "2900 images\n",
      "3000 images\n",
      "3100 images\n",
      "3200 images\n",
      "3300 images\n",
      "3400 images\n",
      "3500 images\n",
      "3600 images\n",
      "3700 images\n",
      "3800 images\n",
      "3900 images\n",
      "4000 images\n",
      "4100 images\n",
      "4200 images\n",
      "4300 images\n",
      "4400 images\n",
      "4500 images\n",
      "4600 images\n",
      "4700 images\n",
      "4800 images\n",
      "4900 images\n",
      "5000 images\n",
      "5100 images\n",
      "5200 images\n",
      "5300 images\n",
      "5400 images\n",
      "5500 images\n",
      "5600 images\n",
      "5700 images\n",
      "5800 images\n",
      "5900 images\n",
      "6000 images\n",
      "6100 images\n",
      "6200 images\n",
      "6300 images\n",
      "6400 images\n",
      "6500 images\n",
      "6600 images\n",
      "6700 images\n",
      "6800 images\n",
      "6900 images\n",
      "7000 images\n",
      "7100 images\n",
      "7200 images\n",
      "7300 images\n",
      "7400 images\n",
      "7500 images\n",
      "7600 images\n",
      "7700 images\n",
      "7800 images\n",
      "7900 images\n",
      "8000 images\n",
      "8100 images\n",
      "8200 images\n",
      "8300 images\n",
      "8400 images\n",
      "8500 images\n",
      "8600 images\n",
      "8700 images\n",
      "8800 images\n",
      "8900 images\n",
      "9000 images\n",
      "9100 images\n",
      "9200 images\n",
      "9300 images\n",
      "9400 images\n",
      "9500 images\n",
      "9600 images\n",
      "9700 images\n",
      "9800 images\n",
      "9900 images\n",
      "Skip\n",
      "10000 images\n",
      "10100 images\n",
      "10200 images\n",
      "10300 images\n",
      "10400 images\n",
      "10500 images\n",
      "10600 images\n",
      "10700 images\n",
      "Skip\n",
      "Extracted all...\n"
     ]
    }
   ],
   "source": [
    "# with tf.device('/GPU:1'):\n",
    "source_file = f\"{root_path}/Data/AesthAI/alm/splitted/alm_train/images/bad/bad7/\"\n",
    "target_file = f\"{root_path}/Data/AesthAI/alm/splitted/alm_train/features/resnet/border_600x600_5k_94_24/\"\n",
    "extract_features_from_path_automated_json(\n",
    "                                 source_file=source_file,\n",
    "                                 target_file=target_file,\n",
    "                                 splitted='bad7',\n",
    "                                 label='bad',\n",
    "                                 resize_func=resize_add_border,\n",
    "                                 size = (600, 600),\n",
    "                                 for_all=False,\n",
    "                                 model=model_resnet, \n",
    "                                 save_json=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991531f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
